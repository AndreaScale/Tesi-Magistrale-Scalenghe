\section{Singular Stochastic Control}

We consider an infinite horizon problem where $O\subset\R^n$, $U\subset\R^n$ a closed cone, functions $\hat{f},\hat{c}\in C^1(\R^n)$ with bounded first order partial derivatives and $\hat{c},\hat{L}\in C(\R^n)$ such that $f(x,v)=v+\hat{f}(x)$, $\sigma(x,v)=\hat{\sigma}(x)$, $L(x,v)=\hat{L}(x)+\hat{c}(v)$ for all $x\in\R^n,v\in U$, and $\hat{c}$ homogeneous of degree one. Furthermore, we set null boundary condition $g\equiv 0$ and non-negative costs. If we then define

\begin{equation}
    \mathcal{H}(t,x,p,A) = \sup_{v\in U}\left[-f(t,x,v)\cdot p - \frac{1}{2}tr(a(t,x,v)A) - L(t,x,v)\right].
\end{equation}

Let $\nu=(\Omega, \{\mathcal{F}_s\}, P, w)$ be a reference system, with $\mathcal{F}_s$ right continuous. We want to define the motion of a state variable through a Stochastic Differential Equation. 
Let us define the auxiliary functions

\begin{equation}
    \hat{u}(s) = \begin{cases}
        \abs{u(s)}^{-1}u(s) & \text{if } u(s) \neq 0 \\
        0 & \text{if } u(s) = 0,
    \end{cases}
\end{equation}

and

\begin{equation}
    \xi(t) = \int_0^t \abs{u(s)} \,ds.
\end{equation}

We thus define the SDE

\begin{equation}\label{4-2-eq: SDE def}
    dx(s) = \hat{f}(x(s))ds + \hat{\sigma}(x(s))dw(s) + \hat{u}(s)d\xi(s),\,s>0.
\end{equation}

What do we consider as a control variable in this context? In classical stochastic optimal control problem the SDE is

\[dx(s) = f(x(s),u(s))ds + \sigma(x(s),u(s))dw(s),\]

where both $f$ and $\sigma$ are possibly time-dependent, and $u$ is the control variable. We define the control variable for \eqref{4-2-eq: SDE def} assign

\begin{equation}\label{4-2-eq: def z}
    z(t) = \int_{[0,t)} \hat{u}(s) \,d\xi(s).
\end{equation}

Since we aim at more general $z(\cdot)$ control functions, that may fail to be absolutely continuous, we impose them to be of bounded variation on every interval $[0,t)$, thus obtaining an almost always differentiable functions. If so, given $\mu(\cdot)$ the total variation of $z(\cdot)$, we get 
\[\xi(t)=\int_{[0,t)} d\mu(s)\]
which is non-decreasing, left-continuous and $\xi(0)=0$. Besides, Radon-Nikodym Theorem implies the existence of a function $\hat{u}(s)$ such that \eqref{4-2-eq: def z} holds. Under this construction $\mathcal{F}_s$-measurability of $z(\cdot)$ is passed to $\xi(\cdot)$ and $\hat{u}(\cdot)$, thus we always assume it. Moreover, we assume that $\hat{u}(s)\in U$ for $\mu$-almost all $s\geq 0$ and that each moment of $z(\cdot)$ is finite, that is $E\abs{z(t)}^m<+\infty$ for all $m\in\N\setminus\{0\}$.
The existence of a unique solution to \eqref{4-2-eq: SDE def} is proven by Picard iteration, although such a $x(\cdot)$ is not necessarily continuous. 

We now want to maximize
\begin{equation}\label{4-2-eq: def J}
    J(x;\xi,\hat{u}) = E_x\int_{[0,\tau)} e^{-\beta s}\left[\hat{L}(x(s))ds + \hat{c}(\hat{u}(s))d\xi(s)\right],
\end{equation}

over all controls $(\xi(\cdot),\hat{u}(\cdot))\in\mathcal{A}_{\nu}$, where $\tau$ is the exit time of $x(s)$ from $\overline{O}$. Finally, to avoid the possibility of $J$ being $+\infty$ we impose 

\begin{equation}
    E_x \int_{[0,\tau)} e^{-\beta s}\abs{L(x(s),u(s))}\,ds<+\infty.
\end{equation}

Thus, the value functions are

\begin{equation}
    V_{\nu}(x) = \inf_{\mathcal{A}_{\nu}} J(x;\xi,\hat{u}),
\end{equation}

and

\begin{equation}
    V(x) = V_{PM}(x) = \inf_{\nu} V_{\nu}.
\end{equation}

We would now be tempted to search for solutions of

\[-\frac{\partial V}{\partial t} + \mathcal{H}(t,x,D_x V,D_x^2 V)=0,\,(t,x)\in Q,\]

but in this context it may be the case that $\mathcal{H}(p)=+\infty$, indeed if 

\begin{equation}\label{4-2-eq: H definition}
    H(p) = \sup_{v\in\hat{K}}-p\cdot v-\hat{c}(v),
\end{equation}

where $\hat{K}$ as the unitary elements of $U$, is strictly positive value then because of homogeneity and $U$ being a cone then

\begin{equation}
    \hat{\mathcal{H}}(p)=\sup_{v\in U}\left\{-p\cdot v - \hat{c}(v)\right\}
\end{equation}

is $+\infty$ there, thus observing that

\[\mathcal{H}(x,p,A) = -\frac{1}{2}trl\left(\hat{a}(x)A\right) - \hat{f}(x)\cdot p - \hat{L}(x) + \hat{\mathcal{H}}(p),\]

we get that $\mathcal{H}$ is $+\infty$. Nevertheless, we expect

\begin{equation}
    H(DV(x)) \leq 0,
\end{equation}

and 

\begin{equation}
    \mathcal{L}V(x) = \beta V(x) - \frac{1}{2}tr\left(\hat{a}(x)D^2V(x)\right) - \hat{f}(x)\cdot DV(x)\leq \hat{L}(x), \, x\in O.
\end{equation}

But if $H(DV(x))<0$ then in a neighborhood of $x$ the optimal control is zero, thus we have

\[\mathcal{L}V(x)=\hat{L}(x).\]

In a more compact notation

\begin{equation}\label{4-2-eq: new dynamic programming}
    \max\left\{\mathcal{L}V(x)-\hat{L}(x),H(DV(x))\right\} = 0,\, x\in O.
\end{equation}

\subsection{Verification Theorem}

The definition of a classical solution of \eqref{4-2-eq: new dynamic programming} is the following.

\begin{definition}
    Let $W\in C_p(\overline{O})\cap C^1(\overline{O})$ with $DW\in L_{loc}^{1,\infty}(O;\R^n)$ and define $\mathcal{P}=\left\{x\in\R^n\,:\,H(DW(x))<0\right\}$. We say that $W$ is classical solution to \eqref{4-2-eq: new dynamic programming} if $W\in C^2(\mathcal{P})$ and $\mathcal{L}W(x)=\hat{L}(x)$ for all $x\in\mathcal{P}$, while $H(DW(x))\leq 0$ for all $x\in\overline{O}$, and $\mathcal{L}W(x)\leq \hat{L}(x)$ almost everywhere in $\R^n$.  
\end{definition}


A verification theorem holds in this context. This result mimics one for classical stochastic optimal control problems, that we have not proved in chapter II [cite IV 5.1].

\begin{theorem}
    Let $O$ be convex and $W$ solution to \eqref{4-2-eq: new dynamic programming} with boundary condition $V(x)=0$ for $x\in\partial O$. Then for all $x\in\overline{O}$:
    
    \begin{enumerate}[label=(\arabic*.)]
        \item for all $(\xi(\cdot),\hat{u}(\cdot))\in\hat{\mathcal{A}}_{\nu}$ such that
        
        \begin{equation}
            \liminf_{t\to+\infty} e^{-\beta t}E_x\left[W(x(t))\chi_{\tau=+\infty}\right] = 0,
        \end{equation}

        then $W(x)\leq J(x;\xi,\hat{u})$

        \item if $W\geq 0$ and there exists $(\xi^{\ast}(\cdot),u^{\ast}(\cdot))\in\hat{\mathcal{A}}_{\nu}$ such that with probability one:
        
        \begin{enumerate}[label=(2.\alph*.)]
            \item \label{4-2: fisrt bullet point} $x^{\ast}(t)\in\mathcal{P}$ for almost every $t\leq \tau$,
            \item \label{4-2: second bullet point} for all $t\leq\tau$
            
            \[\int_{[0,t)} \left[u^{\ast}(s)\cdot DW(x^{\ast}(s)) + \hat{c}(u^{\ast}(s))\right]\,d\xi^{\ast}(s) = 0,\]

            \item \label{4-2: third} for all $t\leq \tau$
            
            \[W(x^{\ast}(t)) - W(x^{\ast}(t^+)) = \hat{c}(u^{\ast}(t))\left[\xi^{\ast}(t^+) - \xi(t)\right],\]
            
            \item \label{4-2: fourth} and 
            
            \[\lim_{t\to+\infty} E_x\left[e^{-\beta(t\land\tau)}W(x^{\ast}(t\land\tau))\chi_{\tau=+\infty}\right] = 0.\]
        \end{enumerate}

        Then $J(x;\xi^{\ast},u^{\ast}) = W(x)$.
    \end{enumerate}

    \begin{proof}
        $(\textit{1}.)$ 

        $(\textit{2}.)$ let us suppose that for some $s\geq0$ we have $x^{\ast}(s)\in\mathcal{P}$. Since $\mathcal{P}$ is open

        \[\mathcal{L}W_m(x^{\ast}(s))\xrightarrow{m\to+\infty}\mathcal{L}W(x^\ast(s))=\hat{L}(x^{ast}(s)).\]

        By \ref{4-2: fisrt bullet point} and the dominated convergence theorem we have

        \begin{equation}
            \lim_{m\to+\infty}\int_0^{t\land\theta_m} e^{-\beta s}\mathcal{L}W_m(x^{\ast}(s))\,ds = E_{x}\int_0^{t\land\theta_N} e^{-\beta}\hat{L}(x^{\ast}(s))\,ds.
        \end{equation}

        If we now impose the optimal control into \eqref{} we get

        \begin{equation}
            \begin{aligned}
                W_m(x) = & E_xe^{-\beta (t\land\tau_m)}W_m(x^{\ast}(t\land\tau_m)) \\
                & + E_x \int_0^{t\land\tau_m} e^{-\beta s} \mathcal{L}W_m(x^{\ast}(s))\,ds \\
                & + E_x \int_0^{t\land\tau_m} e^{-\beta s} \left[-u^{\ast}(s)\cdot DW_m(x^{\ast}(s))\right]d\xi^{\ast,c}(s) \\
                & + E_x \sum_{0\leq s< t\land \tau_m} e^{-\beta s}\left[W_m(x^{ast}(s)) - W_m(x^{\ast}(s^+))\right],
            \end{aligned}
        \end{equation}

        thus letting $m,N,t\to+\infty$ and recalling \ref{4-2: second bullet point}, \ref{4-2: third}, and \ref{4-2: fourth} we get the thesis.
    \end{proof}
\end{theorem}


\section{Viscosity Solution}

Consistently with what has been done in previous chapters we now prove the value function arising from the Singular stochastic problem is indeed a solution (viscous) of the dynamic programming equation. We have already defined the dynamic programming equation in this context, that is equation \eqref{4-2-eq: new dynamic programming}; the value function $V$ is assumed to be in $C_p(\overline{O})$ and satisfy the dynamic programming principle, that is 

\begin{equation}\label{4-2-eq: porgramming principle}
    V(x) = \inf_{\nu} \inf_{\mathcal{A}_{\nu}} \left\{E_xe^{-\beta(\tau\land\theta)}V(x(\tau\land\theta)) + E_x\int_{[0,\tau\land\theta]}e^{-\beta s}\left[\hat{L}(x(s))ds + \hat{c}(\hat{u}(s))d\xi(s)\right]\right\},
\end{equation}

where $\tau$ is the exit time of $x(\cdot)$ from $\overline{O}$ and \eqref{4-2-eq: porgramming principle} holds for all $x\in O$ and all stopping times $\theta$. Formally we can also define the value function as

\begin{equation}
    V(x) = \inf_{\nu}\inf_{\mathcal{A}_{\nu}} J(x;\xi,u),
\end{equation}

recalling what we did in previous chapters.

\begin{theorem}
    Let $V\in C_p(\overline{O})$ such that \eqref{4-2-eq: porgramming principle} holds $\forall x\in O$ and all stopping times $\theta$, then $V$ is a viscosity solution of \eqref{4-2-eq: new dynamic programming}.

    \begin{proof}
        (Subsolution.) Shall we take a $w\in C^2(O)\cap C_p(\overline{O})$ and $\overline{x}\in O$ such that
        
        \[\overline{x} = \arg \max_{\overline{O}} V-w\]

        and show that 

        \begin{equation}\label{4-2-eq: subsol prop}
            \max\left\{\mathcal{L}w(\overline{x}) - \hat{L}(\overline{x}), H(Dw(\overline{x}))\right\} \leq 0.
        \end{equation}

        From \eqref{4-2-eq: porgramming principle:} by taking $\theta = h>0$ we get

        \begin{equation}\label{4-2-eq: subsolution V, relation w and V}
            w(\overline{x}) \leq E_{\overline{x}} \int_{[0,\tau\land h]} e^{-\beta s}\left[\hat{L}(x(s))ds + \hat{c}(\hat{u}(s))d\xi(s)\right]+E_{\overline{x}}e^{-\beta(\tau\land h)}w(x(\tau\land h)).
        \end{equation}

        If we set $\delta>0$, $\xi=\delta\chi_{\R_+}$, and $\hat{u}(s)\equiv \hat{v}\in U$ we get

        \[x(0^+)=\lim_{t\to 0^+}x(t) = x(0) + \hat{u}(0)(\lim_{t\to 0^+}\xi(t)-\xi(0))=\overline{x}+\hat{v}\delta,\]

        then if we let $h\to0^+$ in \eqref{4-2-eq: subsolution V, relation w and V} we get

        \begin{equation}
            w(\overline{x}) \leq \delta\hat{c}(\hat{v}) + w(\overline{x} + \delta\hat{v}),
        \end{equation}

        which implies 

        \begin{equation}
            -Dw(\overline{x}) -\hat{c}(\hat{v}) = \lim_{\delta\to0} \frac{w(\overline{x} + \delta\hat{v}) - w(\overline{x})}{\delta} - \hat{c}(\hat{v}) \leq 0,
        \end{equation}

        for all $\hat{v}\in U$, thus

        \[H(Dw(\overline{x}))\leq 0.\]

        If we instead impose $\xi=0$ in \eqref{4-2-eq: subsolution V, relation w and V} we get
        
        \begin{equation}\label{4-2-eq: first for subsol}
            w(\overline{x}) \leq E_{\overline{x}} \int_{[0,\tau\land h]} e^{-\beta s}\hat{L}(x(s))ds + E_{\overline{x}}e^{-\beta(\tau\land h)}w(x(\tau\land h)),
        \end{equation}    
        
        if we then apply Ito's formula to $e^{-\beta s}w(x(s))$ we get 

        \begin{equation}\label{4-2-eq: second for subsol}
            E_{\overline{x}}e^{-\beta(\tau\land h)}w(x(\tau\land h)) = - E_{\overline{x}} \int_{[0,\tau\land h]} e^{-\beta s}\mathcal{L}w(x(s))ds + w(\overline{x}),
        \end{equation}

        then combining \eqref{4-2-eq: first for subsol} together with \eqref{4-2-eq: second for subsol} we get

        \begin{equation}
            E_{\overline{x}} \int_{[0,\tau\land h]} e^{-\beta s}\mathcal{L}(x(s)) - e^{-\beta s}\hat{L}w(x(s))\,ds\leq 0,  
        \end{equation}

        thus letting $h\to0^+$

        \begin{equation}
            \mathcal{L}w(x(s)) - \hat{L}(x(s))\leq 0.
        \end{equation}

        (Supersolution.) Let $w$ as before and $\overline{x}\in O$ such that $V-w$ is minimized with $(V-w)(\overline{x})=0$, let us show

        \begin{equation}
            \max\left\{\mathcal{L}w(\overline{x}) - \hat{L}(\overline{x}), H(Dw(\overline{x}))\right\} \geq 0.
        \end{equation}

        Let us suppose the contrary, that is the existence of $\delta,\gamma>0$ such that

        \begin{equation}
            \max\left\{\mathcal{L}w(\overline{x}) - \hat{L}(\overline{x}), H(Dw(\overline{x}))\right\} \leq -\gamma, \,\forall x\in B_{\gamma}(\overline{x}).
        \end{equation}

        Let $(\xi(\cdot),\hat{u}(\cdot))\in\hat{\mathcal{A}}_{\nu}$ and $\theta$ the exit time from the ball of $x(\cdot)$, thus $\theta<\tau$. Since $w\in C^2$ then $e^{-\beta s}w(x(s))$ is a semi-martingale, and we can apply Ito's formula to semi-martingales

        \begin{equation}\label{4-2-eq: first w in proof viscosity}
            \begin{aligned}
                w(\overline{x}) = & E_{\overline{x}} e^{-\beta \theta}w(x(\theta)) + E_{\overline{x}}\int_0^{\theta} e^{-\beta s}\mathcal{L}w(x(s))\,ds \\
                & + E_{\overline{x}}\int_0^{\theta} e^{-\beta s}\left[-\hat{u}(s)\cdot Dw(x(s))\right]\,d\xi^c(s) \\
                & + E_{\overline{x}} \sum_{0\leq s <\theta} e^{-\beta s}\left[w(x(s)) - w(x(s^+))\right],
            \end{aligned}
        \end{equation}   

        with $\xi^c(t) = \xi(t) - \sum_{0\leq s <t} \xi(s^+)\xi(s)$. Clearly the absurd condition implies

        \begin{equation}\label{4-2-eq: mathcal L hat L}
            \mathcal{L}w(x(s))\leq\hat{L}(x(s)) - \gamma,\, H(Dw(x(s)))\leq -\gamma,
        \end{equation}

        while we get

        \begin{equation}\label{4-2-eq: h cdot Dw leq gamma}
            -\hat{u}(s)\cdot Dw(x(s)) \leq \hat{c}(\hat{u}(s)) - \gamma\abs{\hat{u}(s)}
        \end{equation}

        by using the latter together with \eqref{4-2-eq: H definition}. Furthermore, by the fundamental theorem of calculus if we set $x_{\rho}=\rho x(s^+)+(1-\rho)x(s)$ for all $\rho\in[0,1]$ we have

        \begin{align*}
            w(x(s)) - w(x(s^+)) & = \int_0^1 -(x(s^+) - x(s))\cdot Dw(\rho x(s^+)+(1-\rho)x(s)) \,d\rho \\
            & = \int_0^1 -(x(s^+) - x(s))\cdot Dw(x_{\rho}) \,d\rho \\
            & \leq \left[\hat{c}(\hat{u}(s) - \gamma\abs{\hat{u}(s)})\right]\left[\xi(s^+) - \xi(s)\right].
        \end{align*}

        Thus, if we combine the latter with \eqref{4-2-eq: h cdot Dw leq gamma} and \eqref{4-2-eq: mathcal L hat L} into \eqref{4-2-eq: first w in proof viscosity} we get

        \begin{align*}
            w(\overline{x}) \leq & E_{\overline{x}} e^{-\beta \theta}w(x(\theta)) \\
            & + E_{\overline{x}} \int_{[0,\theta)} e^{-\beta s}\left[\hat{L}(\hat{u}(s))ds + \hat{c}(\hat{u}(s))d\xi(s)\right] \\
            & - \gamma E_{\overline{x}} \int_{[0,\theta)} e^{-\beta s} \left[ds + \abs{\hat{u}(s)}d\xi(s)\right]. 
        \end{align*}


    \end{proof}
\end{theorem}
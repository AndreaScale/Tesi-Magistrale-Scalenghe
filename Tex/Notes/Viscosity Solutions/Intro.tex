\section{Introduction}

As previously mentioned, in many instances the value function arising from an optimal control problem may fail to be continuously 
differentiable. If that happens the derivation of the Hamilton-Jacobi equation is no longer valid, but more importantly 
the notion of classical solution to it does hold anymore. Therefore, we have to weaken the notion of solution in order to get 
a consistent and unique solution to the dynamic programming equation for non-differentiable value functions. The \textit{viscosity solution} 
is exactly what we are searching for. It arises from a standard procedure called vanishing viscosity, which allows us to compute the 
solution of a fully non-linear first order PDE as the limiting solution of quasilinear parabolic PDEs, obtained via infinitesimal 
perturbations of second order derivatives. 

\subsection{Non-differentiable value functions}

Let us consider the calculus of variation problem:

\begin{equation}
    \inf_{x\in Lip([0,1];[-1,1])} \int_t^{t_1} 1 + \frac{1}{4}(\dot{x}(s))^2 \,ds,
\end{equation}

where $Lip(I;U)$ is the collection of Lipschitz continuous functions from $I$ to $U$. The Hamiltonian related to 
this problem is:

\[H(t,x,p) = \max_{v\in [-1,1]} \left\{-v\cdot p - 1 - \frac{1}{4}v^2\right\}.\]

We can explicitly compute the Hamiltonian and get:

\[H(t,x,p)=p^2-1.\]

Then the Hamilton-Jacobi equations read:

\[\begin{cases}
    \dot{x}^{\ast}(s) = -H_p(s,x^{\ast}(s),p^{\ast}(s)) =  2p^{\ast}(s)\\
    \dot{p}^{\ast}(s) = H_x(s,x^{\ast}(s),p^{\ast}(s)) = 0,
\end{cases}\]

therefore, we get:

\[\dot{x}(s)^{\ast} = 2p^{\ast},\,s\in[0,1],\]

for some $p^{\ast}\in\R$. We now compute the exit time of $(s,x(s))=(s,2(s-t)p^{\ast}+x)$ with initial data $(t,x)$. If $p=0$ then:

\[\tau=1,\,\abs{x}<1.\]

If $p>0$ then $x(s)=2(s-t)p+x$ is increasing, which implies that the system is going to exit from the right boundary, that is from $x(s)=1$, and if 
that happens before time $s=1$ the exit time will be determined by:

\[2(s-t)p+x=1\Rightarrow s=t+\frac{1-x}{2p}.\]

$x(s)=1$ for $s<1$ if:

\[2(1-t)p+x\geq1\Rightarrow p\geq t+\frac{1-x}{2p},\]

therefore:

\[\tau=\begin{cases}
    1 & p\geq t+\frac{1-x}{2p} \\
    t+\frac{1-x}{2p} & p> t+\frac{1-x}{2p}.
\end{cases}\]

Analogously, if $p<0$:

\[\tau=\begin{cases}
    1 & p\leq t-\frac{1+x}{2p} \\
    t-\frac{1+x}{2p} & p< t-\frac{1+x}{2p}.
\end{cases}.\]

We now solve:

\[\inf_{p\in\R} \int_t^{\tau}1+p^2\,ds=\inf_{p\in\R}(1+p^2)(\tau-t)=\begin{cases}
    (1+p^2)(1-t), & p=0\text{ or }p>0\land p\geq \frac{1-x}{2(1-t)}\text{ or }p<0\land p\leq\frac{-1-x}{2(1-t)}\\
    (1+p^2)\frac{1-x}{2p}, & p>0\land p\geq \frac{1-x}{2(1-t)} \\
    -(1+p^2)\frac{1+x}{2p}, & p\leq\frac{-1-x}{2(1-t)},
\end{cases}\]

which is solved as follows:

\[V(t,x)=\begin{cases}
    1-t & \abs{x}\leq t \\
    1-t& \abs{x}\geq t,
\end{cases}\]

which is continuous on the whole space, but clearly no differentiable in $\abs{x}=t$.

\subsection{Vanishing viscosity}

We now euristichally expose a technique called vanishing viscosity, which is widely used in 
calculus of variations problems and will show us one of the origins of the viscosity 
solution notion. Let us consider the initial value problem:

\begin{equation}\label{4-1-fullynonlinear}
    \begin{cases}
        u_t + H(u,Du) = 0,& \R^n\times(0,+\infty) \\
        u = g,& \R^n\times\{t=0\}.
    \end{cases}
\end{equation}

The method of characteristics shows that there cannot be a smooth solution of the above problem over the whole positive 
real line. Indeed, a weaker notion of solution is needed. One approach is to use Hopf-Lax solution concept. We are not 
interested in it, instead we start by perturbing the system as:

\begin{equation}
    \begin{cases}
        u^{\epsilon}_t + H(u^{\epsilon},Du^{\epsilon}) -\epsilon\Delta u^{\epsilon}= 0,& \R^n\times(0,+\infty) \\
        u^{\epsilon} = g,& \R^n\times\{t=0\},
    \end{cases}
\end{equation}

so that the fully non-linear system in \ref{4-1-fullynonlinear} becomes a semilinear one, which turn out to have a smooth solution. 
Then, we take $\epsilon\to0$. We expect the solution $u^{\epsilon}$ to lose the bounds on the derivatives, as they strongly depend on the 
regularization effect of $\epsilon\Lambda$. Turns out that many times Ascoli-Arzela theorem's hypotheses are satisfied, that is $(u^{\epsilon})_{\epsilon}$ 
is uniformly bounded and equicontinuous, then we have local uniform convergence along a subsequence $u^{\epsilon_j}$. We now use the limit
$u\xleftarrow{j\to+\infty}u^{\epsilon_j}$ as a solution. 
We now it to be continuous but we lack information about its derivatives. We will then verify these information using test functions. 
Unlike the classical variational weak solution concept, where integration by part play the central role, we will use the maximum principle 
to translate the derivates of $u$ onto the test functions. 

Let us take $v\in C^{\infty}(\R^n\times(0,+\infty))$ and suppose that $u-v$ has a strict local maximum at $(x_0,t_0)$, then:

\[(u-v)(x_0,t_0) > (u-v)(x,t),\]

for all $(t,x)$ sufficiently close to $(x_0,t_0)$. It can be shown that it implies that there exists $J>0$ such that for all 
$j>J$ there exists $(x_{\epsilon_j},t_{\epsilon_j})$ such that:

\[(u^{\epsilon}-v)(x_{\epsilon_j},t_{\epsilon_j}) \geq (u^{\epsilon}-v)(x,t),\]

for $(x,t)$ sufficiently close to $(x_{\epsilon_j},t_{\epsilon_j})$ and such that:

\[(x_{\epsilon_j},t_{\epsilon_j})\xrightarrow{j\to+\infty}(x_0,t_0).\]

Because $(u^{\epsilon}-v)$ has a local maximum at $(x_{\epsilon_j},t_{\epsilon_j})$:

\[Du^{\epsilon_j}(x_{\epsilon_j},t_{\epsilon_j})=Dv(x_{\epsilon_j},t_{\epsilon_j}),\,u^{\epsilon_j}_t(x_{\epsilon_j},t_{\epsilon_j}) =v(x_{\epsilon_j},t_{\epsilon_j}),\]

and:

\[-\Lambda u^{\epsilon_j}(x_{\epsilon_j},t_{\epsilon_j}) \geq -\Lambda v(x_{\epsilon_j},t_{\epsilon_j}).\]

Therefore, we get:

\[v_t(x_{\epsilon_j},t_{\epsilon_j}) +H(Dv(x_{\epsilon_j},t_{\epsilon_j}) ,x_{\epsilon_j})\leq \Lambda v(x_{\epsilon_j},t_{\epsilon_j}) \xrightarrow{j\to+\infty}0\]

Analogous computations can be done for local minimum of $u-v$, obtaining the opposite inequality above.

We can now grasp the intuition behind the following definition.

\begin{definition}
    A viscosity solution of \ref{4-1-fullynonlinear} is a function $u$ bounded and uniformly continuous on $\R^n\times[0,T]$ for all $T>0$ such that 
    for all $v\in C^{+\infty}(\R^n\times(0,+\infty))$:

    \[v_t(x,t)+H(Dv(x,t),x)\leq0\]

    for all $(x,t)\in\arg \max\{u-v\}$ and:

    \[v_t(x,t)+H(Dv(x,t),x)\geq0\]

    for all $(x,t)\in\arg \min\{u-v\}$. Furthermore, $u\equiv g$ for $t=0$. 
\end{definition}

\subsection{Abstract dynamic programming and viscosity solutions}

We now present an abstraction of the dynamic programming principle, which will allow us to define the viscosity solutions of the dynamic programming 
equation. Let $\Sigma$ be a closed subset of a Banach space and $\mathcal{C}$ a collection of functions on $\Sigma$, closed under addition:

\[\phi,\psi\in\mathcal{C}\Rightarrow \phi+\psi\in\mathcal{C}.\]

We consider the family of operators $\{\mathcal{T}_{tr}\}_{t_0\leq t\leq r\leq t_1}$ such that: 

\begin{equation}\label{4-1-identityofT}
    \mathcal{T}_{tt}\phi = \phi,\, \forall\phi\in\mathcal{C},
\end{equation}

\begin{equation}\label{4-1-leqT}
    \mathcal{T}_{tr}\phi\leq\mathcal{T}_{ts}\psi \text{ if } \phi\leq \mathcal{T}_{rs}\psi,
\end{equation}

and:

\begin{equation}\label{4-1-geqT}
    \mathcal{T}_{tr}\phi\geq\mathcal{T}_{ts}\psi \text{ if } \phi\geq \mathcal{T}_{rs}\psi.
\end{equation}

Conditions \ref{4-1-leqT} and \ref{4-1-geqT} are a weaker version of monotonicity; they imply it together with \ref{4-1-identityofT}. 
Moreover, they also imply the semigroup property, provided that $\mathcal{T}_{rt}:\mathcal{C}\rightarrow\mathcal{C}$. Under this assumption,
the two conditions are equivalent to monotonicity. The semigroup property:

\begin{equation}
    \mathcal{T}_{tr}\left(\mathcal{T}_{rs}\psi\right)=\mathcal{T}_{ts}\psi,\,\mathcal{T}_{rs}\psi\in\mathcal{C},
\end{equation}

is going to be the dynamic programming principle. Indeed, let us consider the classical optimal control problem defined on a bounded set 
$O\subset\R^n$, which we set to be $\Sigma=\overline{O}$ and $\mathcal{C}=\mathcal{M}(\Sigma)$, the collection of measurable functions bounded by below. Then as in chapter 1 we aim at minimize a functional, 
we set this functional to be the operator $\mathcal{T}$. Let us define:

\begin{equation}\label{4-1-valuefunctreform}
    \mathcal{T}_{t,r;u}\psi(x) = \int_t^{\tau\wedge r}L(s,x(s),u(s)),\,ds + g(\tau,x(\tau))\chi_{\tau<r} + \psi(x(r))\chi_{\tau\geq r}, 
\end{equation}

which gives:

\begin{equation}\label{4-1-defofT_dynam}
    \mathcal{T}_{tr}\psi = \inf_{u\in\mathcal{U}(t,x)} \mathcal{T}_{t,r;u}\psi.
\end{equation}

Under the usual assumption on the running and terminal costs, as well as on the control space $U$ we now that the value function defined in ref{4-1-valuefunctreform} 
is measurable and bounded by below. Therefore, $\mathcal{T}_{rt}:\mathcal{C}\rightarrow\mathcal{C}$ and we can formulate the semigroup property just by asking $\psi\in\mathcal{C}$. 
It is clear by its definition that the dynamic programming principle is translated as:

\[\mathcal{T}_{tt_1}\psi(x)=\mathcal{T}_{tr}\left(\mathcal{T}_{rt_1}\psi\right)(x),\]

for $(t,x)\in\overline{Q}$ and $\psi\in\mathcal{C}$. We now derive the abstract dynamic programming equation. The same procedure as in chapter 1 gives:

\[-\frac{1}{h}\left[\mathcal{T}_{tt+h}V(t+h,\cdot)(x)-V(t,x)\right]=0.\]

What happens if we let $h\to0$? We ask for the existence of a family of non-linear operators which will play the role of the Hamiltonian. 
Let $\Sigma'\subset\Sigma$ and $\mathcal{D}\subset C([t_0,t_1)\times\Sigma')$ and $\{\mathcal{G}_t\}_{t\in[t_0,t_1]}$ functions on $\Sigma$ such that:

\begin{equation}\label{4-1-conditiononG_t}
    \lim_{h\to 0}\frac{1}{h}\left[\mathcal{T}_{tt+h}V(t+h,\cdot)(x)-V(t,x)\right] = \frac{\partial}{\partial t}w(t,x) - (\mathcal{G}_tw(t,\cdot))(x)
\end{equation}

for all $w\in\mathcal{D},(t,x)\in Q=[t_0,t_1)\times\Sigma'$. The space $\mathcal{D}$ is such that:

\begin{equation}\label{4-1-conditiononD}
    \forall w\in\mathcal{D} \text{ the functions }\frac{\partial w}{\partial t},\,\mathcal{G}_tw(t,\cdot) \text{ are continuous on $Q$ and }w(t,\cdot)\in\mathcal{C} \forall t\in[t_0,t_1], 
\end{equation}

and $\mathcal{D}$ is a vector space:

\begin{equation}
    w,\tilde{w}\in\mathcal{D} \Rightarrow w+\tilde{w}\in\mathcal{D},\,\lambda w\in\mathcal{D}.
\end{equation}

The elements of $\mathcal{D}$ are called test functions and $\mathcal{G}_t$ the infinitesimal generator of $\mathcal{T}_{tr}$. Explicit choices of $\mathcal{C}$ and 
$\mathcal{D}$ will vary from case to case, usually they are chosen to satisfy certain integrability conditions on the functions. 

If we require the existence of a test function space and an infinitesimal generator of the semigroup given by the value function, the 
dynamic programming equation becomes:

\begin{equation}\label{4-1-newprogreq}
    -\frac{\partial }{\partial t}V(t,x) + (\mathcal{G}_tV(t,\cdot))(x) = 0,\,(t,x)\in Q.
\end{equation}

Then if a function $V\in\mathcal{D}$ satisfies \ref{4-1-newprogreq} point wise it is called a classical solution of it. Thanks to this reformulation we will be able 
to weaken this notion of solution and finally get the viscosity solution of the dynamic programming equation.

We point out that in the canonical deterministic optimal control studied in chapter 1, the infinitesimal generator has the form:

\[(\mathcal{G}_t\phi)(x) = H(t,x,D\phi(x)) = \sup_{v \in U}\left\{-f(t,x,v)\cdot D\phi(x) - L(t,x,v)\right\},\]

with test functions space $\mathcal{D}= C^1(Q)\cap \mathcal{M}(\overline{Q})$ and $\Sigma'=O$.

In view of what we saw in previous sections, we give the definition of viscosity solution. 

\begin{definition}\label{4-1-defviscsol}
    Let $W\in C([t_0,t_1]\times\Sigma)$. Then:
    \begin{enumerate}
        \item $W$ is a \textit{viscosity subsolution} of \ref{4-1-newprogreq} in $Q$ if for every $w\in\mathcal{D}$:
        \begin{equation}\label{4-1-defsubsolution}
            -\frac{\partial}{\partial t}w\left(\overline{t},\overline{x}\right) + \left(\mathcal{G}_{\overline{t}}w\left(\overline{t},\cdot\right)\right)\left(\overline{x}\right) \leq 0,
        \end{equation} 
        at every:
        \[\left(\overline{t},\overline{x}\right)\in \arg \max_{(t,x)\in Q}\left\{(W-w)(t,x)\right\},\]

        and $W\left(\overline{t},\overline{x}\right)=w\left(\overline{t},\overline{x}\right)$.
        \item $W$ is a \textit{viscosity supersolution} of \ref{4-1-newprogreq} in $Q$ if for every $w\in\mathcal{D}$:
        \begin{equation}\label{4-1-defsupersolution}
            -\frac{\partial}{\partial t}w\left(\overline{t},\overline{x}\right) + \left(\mathcal{G}_{\overline{t}}w\left(\overline{t},\cdot\right)\right)\left(\overline{x}\right) \geq 0,
        \end{equation} 
        at every:
        \[\left(\overline{t},\overline{x}\right)\in \arg \min_{(t,x)\in Q}\left\{(W-w)(t,x)\right\},\]

        and $W\left(\overline{t},\overline{x}\right)=w\left(\overline{t},\overline{x}\right)$.
        \item $W$ is a \textit{viscosity solution} if it is both a subsolution and a supersolution.
    \end{enumerate}
\end{definition}

As a first step after the definition of viscosity solutions, we prove its consistency with the classical notion. To do so we require the 
operator to satisfy a maximum principle, that is if $\mathcal{G}_t$ is a general 
operator and:

\[\mathcal{D}=\left\{W\in C([t_0,t_1]\times\Sigma)\,|\,W_t(t,x),(\mathcal{G}_tW(t,\cdot))(x)\in C(Q)\right\},\]

then $\mathcal{G}_t$ satisfies the \textit{maximum principle} if:

\[\mathcal{G}_t\phi(\overline{x})\geq\mathcal{G}_t\psi(\overline{x})\]

for every $\overline{x}\in\arg \max \{(\phi-\psi)(x)\,|\,x\in\Sigma\}\cap\Sigma'$ with $\phi(\overline{x})=\psi(\overline{x})$. Now,
if $W$ is a classical solution of \ref{4-1-newprogreq}, then for a $w\in\mathcal{D}$ and $\left(\overline{t},\overline{x}\right) \in \arg \max_{(t,x)\in Q}\left\{(W-w)(t,x)\right\}$ 
and $W\left(\overline{t},\overline{x}\right)=w\left(\overline{t},\overline{x}\right)$ then:

\begin{align}
    -\frac{\partial}{\partial t}w\left(\overline{t},\overline{x}\right) + \left(\mathcal{G}_{\overline{t}}w\left(\overline{t},\cdot\right)\right)\left(\overline{x}\right) \leq -\frac{\partial}{\partial t}W\left(\overline{t},\overline{x}\right) + \left(\mathcal{G}_{\overline{t}}W\left(\overline{t},\cdot\right)\right)\left(\overline{x}\right) = 0,
\end{align}

since we asked continuity of time derivatives. For the supersolution recall that:

\[ \max \{(\phi-\psi)(x)\,|\,x\in \Sigma\} = \min \{-(\phi-\psi)(x)\,|\,x\in \Sigma\}.\]

If $\mathcal{G}_t$ is the infinitesimal generator of a two-parameter semigroup the connection between classical and viscosity solutions is even stronger.

\begin{proposition}
    Let $W\in\mathcal{D}$. Then $W$ is a classical solution of \ref{4-1-newprogreq} if and only if it is a viscosity solution of \ref{4-1-newprogreq} in $Q$.

    \begin{proof}
        If $W$ is a viscosity solution, since it is also a test function then \ref{4-1-subsolution} and \ref{4-1-supersolution} hold for every point $(t,x)\in Q$, 
        which implies that:
        \[-\frac{\partial}{\partial t}w(t,x) + \left(\mathcal{G}_tw(t,\cdot)\right)(x) = 0,\,\forall(t,x)\in Q.\] 

        If $W$ is a classical solution and prove the subsolution property. Let $w\in\mathcal{D}$ and $(t,x)$ as usual. Since $w\geq W$:

        \begin{align*}
            -\frac{\partial}{\partial t}w(t,x) + (\mathcal{G}_tw(t,\cdot))(x) & = - \lim_{h\to0} \frac{1}{h}\left[(\mathcal{T}_{tt+h}w(t+h,\cdot))(x)-w(t,x)\right] \\
            & \leq - \lim_{h\to0} \frac{1}{h}\left[(\mathcal{T}_{tt+h}W(t+h,\cdot))(x)-W(t,x)\right] \\
            & = -\frac{\partial}{\partial t}W(t,x) + (\mathcal{G}_tW(t,\cdot))(x) = 0.
        \end{align*}

        The supersolution property is proven similarly.
    \end{proof}
\end{proposition}

We now prove that the family of linear operators $\mathcal{T}_{tt_1}$ defined in \ref{4-1-defofT_dynam} defines viscosity solution of the dynamic programming 
equation \ref{4-1-newprogreq}.

\begin{theorem}\label{4-1-thm:Vtoviscosity}
    Let $\{\mathcal{T}_{tr}\}_{t_0\leq t\leq r\leq t_1}$ such that \ref{4-1-identityofT},\ref{4-1-geqT},\ref{4-1-leqT} and also there exists a vector space 
    $\mathcal{D}$ and another family of operator $\{\mathcal{G}_t\}_{t\in[t_0,t_1]}$ such that \ref{4-1-conditiononD} and \ref{4-1-conditiononG_t} hold. Let:
    
    \[V(t,x)=\left(\mathcal{T}_{tt_1}\psi\right)(x).\]

    If $V\in C(Q)$ then it is a viscosity solution of \ref{4-1-newprogreq}.

    \begin{proof}
        Let us prove the subsolution condition. Let $w\in\mathcal{D}$ and $(t,x)$ maximizer of $V-w$ in $\overline{Q}$ and $V(t,x)=w(t,x)$. Then $V\leq w$ on 
        $Q$, which implies by \ref{4-1-geqT} that we have:
        
        \begin{equation}\label{4-1-proof1-firsstineq}(\mathcal{T}_{tr}w(r,\cdot))(x) \geq (\mathcal{T}_{tt_1}\psi)(x) = V(t,x) = w(t,x).\end{equation}

        Now, because of \ref{4-1-conditiononD} and \ref{4-1-conditiononG_t} we can compute:
        
        \[-\frac{\partial}{\partial t}w(t,x) + (\mathcal{G}_tw(t,\cdot))(x) = - \lim_{h\to0} \frac{1}{h}\left[(\mathcal{T}_{tt+h}w(t+h,\cdot))(x)-w(t,x)\right]\leq0\]

        because of \ref{4-1-proof1-firsstineq}. The supersolution condition is proven analogously.
    \end{proof}
\end{theorem}

\subsection{The Standard Approach}

We have talked about \textit{viscosity solutions} for $\mathcal{G}_t$ that are infinitesimal generators of semigroups. Historically the notion of viscosity solution was introduced 
for partial differential equations, that is when $\mathcal{G}_t$ is a partial differential operator. In this section we will introduce this classical notion and provide some properties, 
establishing as well the relevant links to the already proven theory. 

Let $\mathcal{G}_t$ be a partial differential operator defined As

\begin{equation}\label{4-1-GPDEop}
    (\mathcal{G}_t\phi)(x) = F(t,x,D\phi(x), D^2\phi(x), \phi(x)),
\end{equation}

where $F$ is taken to be continuous. We assume that $\sigma'=O\subset\R^n$ open and $\sigma$ to be its closure. 
Furthermore, we assume that test functions $\mathcal{C}$ and $\mathcal{D}$ are such that

\begin{equation}\label{4-1-condonCandD}
    C_p(\overline{O})\cap\mathcal{M}(\overline{O}) \subset \mathcal{C},\,C_p^{\infty}(\overline{Q})\cap\mathcal{M}(\overline{Q}) \subset \mathcal{D},
\end{equation}

where $Q=[t_0,t_1)\times O$ as usual. A useful result is the following.

\begin{lemma}
    The operator $\mathcal{G}_t$ obeys the maximum principle if and only if $F$ is elliptic.

    \begin{proof}
        Recall that $F$ is elliptic If

        \begin{equation}\label{4-1-defelliptic}
            F(t,x,p,A+B,V) \leq F(t,x,p,A,V)
        \end{equation}

        for all $(t,x,p,V)\in Q\times\R^n\times\R$ and all $A,B\in Sym(\R^{n\times n})$ with $B\geq0$.

        Let us assume that $F$ is elliptic, then for generic $\phi,\psi\in C^2(O)$ and $\overline{x}\in\arg \max\{(\phi-\psi)(x)|x\in\overline{O}\}\cap O$ 
        with $\phi(\overline{x})=\psi(\overline{x})$ we have that $D\phi(\overline{x})=D\psi(\overline{x})$ and $D^2\phi(\overleftarrow{x})\leq D^2\psi(\overleftarrow{x})$, 
        therefore

        \[F(t,\overline{x}, D\phi(\overline{x}), D^2\phi(\overline{x}),\phi(\overline{x}))\geq F(t,\overline{x}, D\psi(\overline{x}), D^2\psi(\overline{x}),\psi(\overline{x})),\]

        just by using \ref{4-1-defelliptic} with $B=D^\psi(\overline{x})-D^2\phi(\overline{x})\geq0$. If $F$ is not elliptic 
        for some $(t,\overline{x},p,A,V)$ then the functions

        \[\psi(x) = V+p\cdot(x-\overline{x}) + \frac{1}{2}(A+B)(x-\overline{x})\cdot(x-\overline{x}),\]

        \[\phi(x) = V+p\cdot(x-\overline{x}) + \frac{1}{2}A(x-\overline{x})\cdot(x-\overline{x}),\]

        for $x\in\overline{O}$ are such that $\overline{x}$ maximizes the distance between them, they coincide over it, but 
        the value of $F$ in $\overline{x}$ and $\phi,\psi$ does not satisfy the maximum principle, since \ref{4-1-defelliptic} 
        does not hold.
    \end{proof}
\end{lemma}

I this context we give the following definition.

\begin{definition}\label{4-1-defviscforW}
    Given a function $W\in C(\overline{O})$ we define the equation
    \begin{equation}\label{4-1-PDEwithW}
        -\frac{\partial}{\partial t}W(t,x) + F(t,x,D_xW(t,x),D_x^2W(t,x),W(t,x))=0
    \end{equation}

    \begin{enumerate}[label=(\alph*)]
        \item $W$ is a \textit{viscosity subsolution} of \ref{4-1-PDEwithW} in $Q$ if for each $w\in C^{\infty}(Q)$ then 
        
        \begin{equation}
            -\frac{\partial}{\partial t}w(t,x) + F(t,x,D_xw(t,x),D_x^2w(t,x),w(t,x))\leq0
        \end{equation}

        at every $(t,x)\in Q$ which locally maximizes $W-w$ yielding $0$.
        
        \item $W$ is a \textit{viscosity supersolution} of \ref{4-1-PDEwithW} in $Q$ if for each $w\in C^{\infty}(Q)$ then 
        
        \begin{equation}
            -\frac{\partial}{\partial t}w(t,x) + F(t,x,D_xw(t,x),D_x^2w(t,x),w(t,x))\geq0
        \end{equation}

        at every $(t,x)\in Q$ which locally minimizes $W-w$ yielding $0$.

        \item $W$ is a \textit{viscosity solution} if it is both a viscosity subsolution and supersolution.
    \end{enumerate}
\end{definition}

We will prove that definitions \ref{4-1-defviscsol} and \ref{4-1-defviscforW} are equivalent for partial differential operators 
of the form \ref{4-1-GPDEop}, provided that the function has sub-polynomial growth and is measurable.

An handy way of verifying both the definitions is the following.

\begin{lemma}
    If the conditions in the definitions \ref{4-1-defviscsol}, \ref{4-1-defviscforW} is satisfied only at strict extrema 
    then it is satisfied at every extrema.

    \begin{proof}
        We prove it for the minimum of definition \ref{4-1-defviscforW}, the other assertions are proved similarly. Let 
        $(t,x)$ be a non strict minimum of $W-w$ such that $W(t,x)=w(t,x)$. Let $\epsilon>0$ and deifne $w^{\epsilon}=w+\epsilon\xi$ where
        
        \[\xi(s,y)=e^{-(\abs{s-t}^2+\abs{y-x}^2)}-1\]

        defined over $\overline{Q}$. Since $\mathcal{D}$ is a vector space and $\epsilon\xi\in\mathcal{D}$ by \ref{4-1-condonCandD}, then 
        $w^{\epsilon}\in\mathcal{D}$. Then for every $\epsilon>0$ the function $W-w^{\epsilon}$ has a strict minimum at $(t,x)$ with $W(t,x)=w^{\epsilon}(t,x)$ which 
        implies that
        
        \[-\frac{\partial}{\partial t}w^{\epsilon}(t,x) + F(t,x,D_xw^{\epsilon}(t,x),D_x^2w^{\epsilon}(t,x),w^{\epsilon}(t,x))\leq0.\]

        By continuity of $F$ and smoothness of $w^{\epsilon}$ we let $\epsilon\to0$ and get the wanted inequality.
    \end{proof}
\end{lemma}


Finally, we prove the equivalence of these definitions. 

\begin{theorem}
    Let all the previous assumptions and $W\in C_p(\overline{Q})\cap\mathcal{M}(\overline{Q})$ and $\mathcal{D}\subset C^{1,2}(Q)$. Then $W$ is a subsolution (supersolution) 
    of \ref{4-1-PDEwithW} as defined by \ref{4-1-defviscsol} if and only if it is a subsolution (supersolution) as defined by \ref{4-1-defviscforW}. 

    \begin{proof}
        Let $W$ be a supersolution as in \ref{4-1-defviscsol}. We just prove the supersolution property, since if we have $W$ subsolution we can see it as 
        a supersolution. Let $W$ a subsolution and $(t,x)$ a strict maximum of $W-w$ and $W(t,x)=w(t,x)$, then we can find an open set $A$ such that $w>W$ over 
        $A\cap Q$. By sub-polynomial growth there exists a constant $K$ such that 

        \[\abs{W(s,y)} \leq K(1+\abs{y}^{2m}),\,\forall (s,y)\in\overline{Q}\]

        for some $m>0$. Then for some $\xi$ given by Urysohn’s lemma we can define 

        \[\overline{w}(s,y)= \xi(s,y)w(s,y) - (1-\xi(s,y))K(1+\abs{y}^{2m}),\]

        and get a minimum for $W-\overline{w}$ in $(t,x)$. Then we are in the supersolution case.

        Let $w\in C^{\infty}(Q)$ and $(t,x)$ a local minimum of $W-w$ where it is null. There exists an open subset $\mathcal{N}$ of $\R^n$ such that 
        $W\geq w$ over $\mathcal{N}\cap\overline{Q}$, and by Urysohn’s lemma there exists a function $\xi\in C^{\infty}(\overline{Q})$ taking values in $[0,1]$, 
        constantly $1$ in a neighborhood $\tilde{\mathcal{N}}$ of $(t,x)$ and zero outside $\mathcal{N}$. Since $W$ is bounded below we define 
        
        \begin{equation}
            \tilde{w}(s,y) = \xi(s,y)w(s,y) - (1-\xi(s,y))K,
        \end{equation}

        where $-K$ is the lower bound of $W$. Then, by hypotheses on $W$, $w$, and $\xi$ we have

        \[W = \xi W  + (1-\xi) W \geq \xi w + (1-\xi)W\geq\xi w-(1-\xi)K=\overline{w}.\]

        Furthermore, $\overline{w}\in\mathcal{D}$ since \ref{4-1-condonCandD} holds, $(t,x)$ is a minimum of $W-\overline{w}$, and $W(t,x)=\overline{w}(t,x)$ 
        then 
        
        \[-\frac{\partial}{\partial t}\overline{w}(t,x) + F(t,x,D_y\overline{w}(t,x),D_y^2\overline{w}(t,x),\overline{w}(t,x))\geq0,\]

        but $\overline{w}$ and $w$ have same derivative in $(t,x)$, since $\xi$ is constantly unitary in a neighborhood of it. Then we get

        \[-\frac{\partial}{\partial t}w(t,x) + F(t,x,D_yw(t,x),D_y^2w(t,x),w(t,x))\geq0.\]

        Let us suppose that $W$ is a subsolution for \ref{4-1-defviscforW}. Let $w\in\mathcal{D}$ and $(t,x)$ a strict maximum of $W-w$ such that $(W-w)(t,x)=0$. 
        Since $\mathcal{D}\subset C^{1,2}(Q)$ we can find an open set $Q^{\ast}$ over which $w$ is $C^{1,2}$ and then extend it to be null over $\R^{n+1}\setminus Q^{\ast}$. 
        Let us define a sequence of mollifications of $w$ defined as $w^n$ such that $w^n,w_t^n,w_{x_i}^n,w_{x_ix_j}^n$ converge to their respective of $w$. 
        Since the convergence of $w^n$ to $w$ is uniform there must be a subsequence $(t_n,x_n)$ converging to $(t,x)$, such that $W-w^n$ has a maximum. If we set 
        $\overline{w}^n=w^n-w^n(t_n,x_n)+W(t_n,x_n)$ we still have a maximum in $(t_n,x_n)$ for $W-\overline{w}^n$ and $W(t_n,x_n)=\overline{w}^n(t_n,x_n)$. Then 

        \[-\frac{\partial}{\partial t}\overline{w}^n(t_n,x_n) + F(t_n,x_n,D_x\overline{w}^n(t_n,x_n), D_x^2\overline{w}^n(t_n,x_n), \overline{w}^n(t_n,x_n))\leq0.\]

        Because of continuity of $F$ and smoothness of $\overline{w}^n$ we get
        
        \[-\frac{\partial}{\partial t}w(t,x) + F(t,x,D_yw(t,x),D_y^2w(t,x),w(t,x))\leq0.\]
    \end{proof}
\end{theorem}

\subsection{Value Function as Viscosity Solution}

In the first section we proved that the value function of a dynamic programming problem is the classical solution of the dynamic programming equation, under differentiability 
assumptions. We now prove that it is a viscosity solution of the dynamic programming equation given two sets of assumptions.

\begin{theorem}\label{4-1-viscodynamiceq}
    Let $U$ be a bounded space of control and $f\in C(\overline{Q}\times U)$ such that $\abs{f(t,x,v)}\leq K(1+\abs{x})$. The for every $w\in C^1(Q)\cap \mathcal{M}(\overline{Q})$ 

    \begin{equation}
        \lim_{h\to 0} \frac{1}{h}\left[(\mathcal{T}_{tt+h}w(t+h,\cdot))(x)-w(t,x)\right] = \frac{\partial}{\partial t}w(t,x) - H(t,x,D_xw(t,x)), \,\forall(t,x)\in\overline{Q}.
    \end{equation}
\end{theorem}

\vspace{5mm}


Note that equation \ref{4-1-viscodynamiceq} implies that $V$ is a viscosity solution of \ref{1-2-HJB1} via theorem \ref{4-1-thm:Vtoviscosity}. Indeed, 
there exists an admissible control $u(\cdot)\in \mathcal{U}(t,x)$ such that $\lim_{s\to t}u(s)=v$. Let $x(\cdot)$ be the state solution of the Cauchy problem associated 
to the control $u(\cdot)$ such that $x(t)=x$. Let also $\tau$ the exit time of $(s,x(s))$ from $Q$. If we take a sufficiently small $h>0$ we can compute

\begin{align*}
    \frac{1}{h}\left[(\mathcal{T}_{tt+h}w(t+h,\cdot))(x)-w(t,x)\right] & \leq \frac{1}{h}\int_t^{t+h}L(s,x(s),u(s))\,ds + \frac{1}{h}\left[w(t+h,x(t+h))-w(t,x)\right] \\
    & =  \frac{1}{h}\int_t^{t+h}\bigg[L(s,x(s),u(s)) \\ 
    & \quad + \frac{\partial}{\partial t}w(s,x(s)) + f(s,x(s),u(s))\cdot D_xw(s,x(s))\bigg]\,ds.
\end{align*}

Then 

\[\limsup_{h\to0}\frac{1}{h}\left[(\mathcal{T}_{tt+h}w(t+h,\cdot))(x)-w(t,x)\right]\leq\frac{\partial}{\partial t}w(t,x) - H(t,x,D_xw(t,x)).\]

\vspace{5mm}

\begin{proof}

    Let $(t,x)\in Q$ and $u(\cdot)\in\mathcal{U}(t,x)$ then using $\abs{f(t,x,v)}\leq K(1+\abs{x})$

    \begin{align*}
        \abs{x(r)-x} & \leq \int_r^t \abs{f(s,x(s),u(s))} \,ds \\
        & \leq K(r-t) + K\int_t^r \abs{x}(s)\,ds \\
        & \leq K(r-t)(1+\abs{x}) + K\int_t^r \abs{x(s)-x}\,ds,
    \end{align*}

    which implies by Gronwall's inequality 

    \[\abs{x(r)-x} \leq (1+\abs{x})e^{K(t-r)-1},\,\forall r\geq t.\]

    Then for suitably small $r$ that $\tau\geq r\wedge t_1$ for all $u(\cdot)\in\mathcal{U}(t,x)$. Then for $n$ sufficiently large
\end{proof}



FARE di II.6 tutto fino a p.76. Poi fare II.7, in particolare theorem 7.1 bene. Anche 7.2. (tutto insomma)

Poi II.8-II.9 da fare con criterio. Lemmi super tecnici dati per buoni. Fare bene unicità (molto lunga).

Poi II.10 fare bene Lipschitz.

Ci vuole importante e chiara spiegazione su perchè essere lipshizt sia fondamentale.

Poi II.15 Prontryagin.

La discussione sulle condizioni al constorno e robe varie boh. Vediamo come procede. Le cose prima necessarie.  
\section{Introduction}

As previously mentioned, in many instances the value function arising from an optimal control problem may fail to be continuously 
differentiable. If that happens the derivation of the Hamilton-Jacobi equation is no longer valid, but more importantly 
the notion of classical solution to it does hold anymore. Therefore, we have to weaken the notion of solution in order to get 
a consistent and unique solution to the dynamic programming equation for non-differentiable value functions. The \textit{viscosity solution} 
is exactly what we are searching for. It arises from a standard procedure called vanishing viscosity, which allows us to compute the 
solution of a fully non-linear first order PDE as the limiting solution of quasilinear parabolic PDEs, obtained via infinitesimal 
perturbations of second order derivatives. 

\subsection{Non-differentiable value functions}

Let us consider the calculus of variation problem:

\begin{equation}
    \inf_{x\in Lip([0,1];[-1,1])} \int_t^{t_1} 1 + \frac{1}{4}(\dot{x}(s))^2 \,ds,
\end{equation}

where $Lip(I;U)$ is the collection of Lipschitz continuous functions from $I$ to $U$. The Hamiltonian related to 
this problem is:

\[H(t,x,p) = \max_{v\in [-1,1]} \left\{-v\cdot p - 1 - \frac{1}{4}v^2\right\}.\]

We can explicitly compute the Hamiltonian and get:

\[H(t,x,p)=p^2-1.\]

Then the Hamilton-Jacobi equations read:

\[\begin{cases}
    \dot{x}^{\ast}(s) = -H_p(s,x^{\ast}(s),p^{\ast}(s)) =  2p^{\ast}(s)\\
    \dot{p}^{\ast}(s) = H_x(s,x^{\ast}(s),p^{\ast}(s)) = 0,
\end{cases}\]

therefore, we get:

\[\dot{x}(s)^{\ast} = 2p^{\ast},\,s\in[0,1],\]

for some $p^{\ast}\in\R$. We now compute the exit time of $(s,x(s))=(s,2(s-t)p^{\ast}+x)$ with initial data $(t,x)$. If $p=0$ then:

\[\tau=1,\,\abs{x}<1.\]

If $p>0$ then $x(s)=2(s-t)p+x$ is increasing, which implies that the system is going to exit from the right boundary, that is from $x(s)=1$, and if 
that happens before time $s=1$ the exit time will be determined by:

\[2(s-t)p+x=1\Rightarrow s=t+\frac{1-x}{2p}.\]

$x(s)=1$ for $s<1$ if:

\[2(1-t)p+x\geq1\Rightarrow p\geq t+\frac{1-x}{2p},\]

therefore:

\[\tau=\begin{cases}
    1 & p\geq t+\frac{1-x}{2p} \\
    t+\frac{1-x}{2p} & p> t+\frac{1-x}{2p}.
\end{cases}\]

Analogously, if $p<0$:

\[\tau=\begin{cases}
    1 & p\leq t-\frac{1+x}{2p} \\
    t-\frac{1+x}{2p} & p< t-\frac{1+x}{2p}.
\end{cases}.\]

We now solve:

\[\inf_{p\in\R} \int_t^{\tau}1+p^2\,ds=\inf_{p\in\R}(1+p^2)(\tau-t)=\begin{cases}
    (1+p^2)(1-t), & p=0\text{ or }p>0\land p\geq \frac{1-x}{2(1-t)}\text{ or }p<0\land p\leq\frac{-1-x}{2(1-t)}\\
    (1+p^2)\frac{1-x}{2p}, & p>0\land p\geq \frac{1-x}{2(1-t)} \\
    -(1+p^2)\frac{1+x}{2p}, & p\leq\frac{-1-x}{2(1-t)},
\end{cases}\]

which is solved as follows:

\[V(t,x)=\begin{cases}
    1-t & \abs{x}\leq t \\
    1-t& \abs{x}\geq t,
\end{cases}\]

which is continuous on the whole space, but clearly no differentiable in $\abs{x}=t$.

\subsection{Vanishing viscosity}

We now euristichally expose a technique called vanishing viscosity, which is widely used in 
calculus of variations problems and will show us one of the origins of the viscosity 
solution notion. Let us consider the initial value problem:

\begin{equation}\label{4-1-fullynonlinear}
    \begin{cases}
        u_t + H(u,Du) = 0,& \R^n\times(0,+\infty) \\
        u = g,& \R^n\times\{t=0\}.
    \end{cases}
\end{equation}

The method of characteristics shows that there cannot be a smooth solution of the above problem over the whole positive 
real line. Indeed, a weaker notion of solution is needed. One approach is to use Hopf-Lax solution concept. We are not 
interested in it, instead we start by perturbing the system as:

\begin{equation}
    \begin{cases}
        u^{\epsilon}_t + H(u^{\epsilon},Du^{\epsilon}) -\epsilon\Delta u^{\epsilon}= 0,& \R^n\times(0,+\infty) \\
        u^{\epsilon} = g,& \R^n\times\{t=0\},
    \end{cases}
\end{equation}

so that the fully non-linear system in \ref{4-1-fullynonlinear} becomes a semilinear one, which turn out to have a smooth solution. 
Then, we take $\epsilon\to0$. We expect the solution $u^{\epsilon}$ to lose the bounds on the derivatives, as they strongly depend on the 
regularization effect of $\epsilon\Lambda$. Turns out that many times Ascoli-Arzela theorem's hypotheses are satisfied, that is $(u^{\epsilon})_{\epsilon}$ 
is uniformly bounded and equicontinuous, then we have local uniform convergence along a subsequence $u^{\epsilon_j}$. We now use the limit
$u\xleftarrow{j\to+\infty}u^{\epsilon_j}$ as a solution. 
We now it to be continuous but we lack information about its derivatives. We will then verify these information using test functions. 
Unlike the classical variational weak solution concept, where integration by part play the central role, we will use the maximum principle 
to translate the derivates of $u$ onto the test functions. 

Let us take $v\in C^{\infty}(\R^n\times(0,+\infty))$ and suppose that $u-v$ has a strict local maximum at $(x_0,t_0)$, then:

\[(u-v)(x_0,t_0) > (u-v)(x,t),\]

for all $(t,x)$ sufficiently close to $(x_0,t_0)$. It can be shown that it implies that there exists $J>0$ such that for all 
$j>J$ there exists $(x_{\epsilon_j},t_{\epsilon_j})$ such that:

\[(u^{\epsilon}-v)(x_{\epsilon_j},t_{\epsilon_j}) \geq (u^{\epsilon}-v)(x,t),\]

for $(x,t)$ sufficiently close to $(x_{\epsilon_j},t_{\epsilon_j})$ and such that:

\[(x_{\epsilon_j},t_{\epsilon_j})\xrightarrow{j\to+\infty}(x_0,t_0).\]

Because $(u^{\epsilon}-v)$ has a local maximum at $(x_{\epsilon_j},t_{\epsilon_j})$:

\[Du^{\epsilon_j}(x_{\epsilon_j},t_{\epsilon_j})=Dv(x_{\epsilon_j},t_{\epsilon_j}),\,u^{\epsilon_j}_t(x_{\epsilon_j},t_{\epsilon_j}) =v(x_{\epsilon_j},t_{\epsilon_j}),\]

and:

\[-\Lambda u^{\epsilon_j}(x_{\epsilon_j},t_{\epsilon_j}) \geq -\Lambda v(x_{\epsilon_j},t_{\epsilon_j}).\]

Therefore, we get:

\[v_t(x_{\epsilon_j},t_{\epsilon_j}) +H(Dv(x_{\epsilon_j},t_{\epsilon_j}) ,x_{\epsilon_j})\leq \Lambda v(x_{\epsilon_j},t_{\epsilon_j}) \xrightarrow{j\to+\infty}0\]

Analogous computations can be done for local minimum of $u-v$, obtaining the opposite inequality above.

We can now grasp the intuition behind the following definition.

\begin{definition}
    A viscosity solution of \ref{4-1-fullynonlinear} is a function $u$ bounded and uniformly continuous on $\R^n\times[0,T]$ for all $T>0$ such that 
    for all $v\in C^{+\infty}(\R^n\times(0,+\infty))$:

    \[v_t(x,t)+H(Dv(x,t),x)\leq0\]

    for all $(x,t)\in\arg \max\{u-v\}$ and:

    \[v_t(x,t)+H(Dv(x,t),x)\geq0\]

    for all $(x,t)\in\arg \min\{u-v\}$. Furthermore, $u\equiv g$ for $t=0$. 
\end{definition}

\subsection{Abstract dynamic programming and viscosity solutions}

We now present an abstraction of the dynamic programming principle, which will allow us to define the viscosity solutions of the dynamic programming 
equation. Let $\Sigma$ be a closed subset of a Banach space and $\mathcal{C}$ a collection of functions on $\Sigma$, closed under addition:

\[\phi,\psi\in\mathcal{C}\Rightarrow \phi+\psi\in\mathcal{C}.\]

We consider the family of operators $\{\mathcal{T}_{tr}\}_{t_0\leq t\leq r\leq t_1}$ such that: 

\begin{equation}\label{4-1-identityofT}
    \mathcal{T}_{tt}\phi = \phi,\, \forall\phi\in\mathcal{C},
\end{equation}

\begin{equation}\label{4-1-leqT}
    \mathcal{T}_{tr}\phi\leq\mathcal{T}_{ts}\psi \text{ if } \phi\leq \mathcal{T}_{rs}\psi,
\end{equation}

and:

\begin{equation}\label{4-1-geqT}
    \mathcal{T}_{tr}\phi\geq\mathcal{T}_{ts}\psi \text{ if } \phi\geq \mathcal{T}_{rs}\psi.
\end{equation}

Conditions \ref{4-1-leqT} and \ref{4-1-geqT} are a weaker version of monotonicity; they imply it together with \ref{4-1-identityofT}. 
Moreover, they also imply the semigroup property, provided that $\mathcal{T}_{rt}:\mathcal{C}\rightarrow\mathcal{C}$. Under this assumption,
the two conditions are equivalent to monotonicity. The semigroup property:

\begin{equation}
    \mathcal{T}_{tr}\left(\mathcal{T}_{rs}\psi\right)=\mathcal{T}_{ts}\psi,\,\mathcal{T}_{rs}\psi\in\mathcal{C},
\end{equation}

is going to be the dynamic programming principle. Indeed, let us consider the classical optimal control problem defined on a bounded set 
$O\subset\R^n$, which we set to be $\Sigma=\overline{O}$ and $\mathcal{C}=\mathcal{M}(\Sigma)$, the collection of measurable functions bounded by below. Then as in chapter 1 we aim at minimize a functional, 
we set this functional to be the operator $\mathcal{T}$. Let us define:

\begin{equation}\label{4-1-valuefunctreform}
    \mathcal{T}_{t,r;u}\psi(x) = \int_t^{\tau\wedge r}L(s,x(s),u(s)),\,ds + g(\tau,x(\tau))\chi_{\tau<r} + \psi(x(r))\chi_{\tau\geq r}, 
\end{equation}

which gives:

\begin{equation}\label{4-1-defofT_dynam}
    \mathcal{T}_{tr}\psi = \inf_{u\in\mathcal{U}(t,x)} \mathcal{T}_{t,r;u}\psi.
\end{equation}

Under the usual assumption on the running and terminal costs, as well as on the control space $U$ we now that the value function defined in ref{4-1-valuefunctreform} 
is measurable and bounded by below. Therefore, $\mathcal{T}_{rt}:\mathcal{C}\rightarrow\mathcal{C}$ and we can formulate the semigroup property just by asking $\psi\in\mathcal{C}$. 
It is clear by its definition that the dynamic programming principle is translated as:

\[\mathcal{T}_{tt_1}\psi(x)=\mathcal{T}_{tr}\left(\mathcal{T}_{rt_1}\psi\right)(x),\]

for $(t,x)\in\overline{Q}$ and $\psi\in\mathcal{C}$. We now derive the abstract dynamic programming equation. The same procedure as in chapter 1 gives:

\[-\frac{1}{h}\left[\mathcal{T}_{tt+h}V(t+h,\cdot)(x)-V(t,x)\right]=0.\]

What happens if we let $h\to0$? We ask for the existence of a family of non-linear operators which will play the role of the Hamiltonian. 
Let $\Sigma'\subset\Sigma$ and $\mathcal{D}\subset C([t_0,t_1)\times\Sigma')$ and $\{\mathcal{G}_t\}_{t\in[t_0,t_1]}$ functions on $\Sigma$ such that:

\begin{equation}\label{4-1-conditiononG_t}
    \lim_{h\to 0}\frac{1}{h}\left[\mathcal{T}_{tt+h}V(t+h,\cdot)(x)-V(t,x)\right] = \frac{\partial}{\partial t}w(t,x) - (\mathcal{G}_tw(t,\cdot))(x)
\end{equation}

for all $w\in\mathcal{D},(t,x)\in Q=[t_0,t_1)\times\Sigma'$. The space $\mathcal{D}$ is such that:

\begin{equation}\label{4-1-conditiononD}
    \forall w\in\mathcal{D} \text{ the functions }\frac{\partial w}{\partial t},\,\mathcal{G}_tw(t,\cdot) \text{ are continuous on $Q$ and }w(t,\cdot)\in\mathcal{C} \forall t\in[t_0,t_1], 
\end{equation}

and $\mathcal{D}$ is a vector space:

\begin{equation}
    w,\tilde{w}\in\mathcal{D} \Rightarrow w+\tilde{w}\in\mathcal{D},\,\lambda w\in\mathcal{D}.
\end{equation}

The elements of $\mathcal{D}$ are called test functions and $\mathcal{G}_t$ the infinitesimal generator of $\mathcal{T}_{tr}$. Explicit choices of $\mathcal{C}$ and 
$\mathcal{D}$ will vary from case to case, usually they are chosen to satisfy certain integrability conditions on the functions. 

If we require the existence of a test function space and an infinitesimal generator of the semigroup given by the value function, the 
dynamic programming equation becomes:

\begin{equation}\label{4-1-newprogreq}
    -\frac{\partial }{\partial t}V(t,x) + (\mathcal{G}_tV(t,\cdot))(x) = 0,\,(t,x)\in Q.
\end{equation}

Then if a function $V\in\mathcal{D}$ satisfies \ref{4-1-newprogreq} point wise it is called a classical solution of it. Thanks to this reformulation we will be able 
to weaken this notion of solution and finally get the viscosity solution of the dynamic programming equation.

We point out that in the canonical deterministic optimal control studied in chapter 1, the infinitesimal generator has the form:

\[(\mathcal{G}_t\phi)(x) = H(t,x,D\phi(x)) = \sup_{v \in U}\left\{-f(t,x,v)\cdot D\phi(x) - L(t,x,v)\right\},\]

with test functions space $\mathcal{D}= C^1(Q)\cap \mathcal{M}(\overline{Q})$ and $\Sigma'=O$.

In view of what we saw in previous sections, we give the definition of viscosity solution. 

\begin{definition}\label{4-1-defviscsol}
    Let $W\in C([t_0,t_1]\times\Sigma)$. Then:
    \begin{enumerate}
        \item $W$ is a \textit{viscosity subsolution} of \ref{4-1-newprogreq} in $Q$ if for every $w\in\mathcal{D}$:
        \begin{equation}\label{4-1-defsubsolution}
            -\frac{\partial}{\partial t}w\left(\overline{t},\overline{x}\right) + \left(\mathcal{G}_{\overline{t}}w\left(\overline{t},\cdot\right)\right)\left(\overline{x}\right) \leq 0,
        \end{equation} 
        at every:
        \[\left(\overline{t},\overline{x}\right)\in \arg \max_{(t,x)\in Q}\left\{(W-w)(t,x)\right\},\]

        and $W\left(\overline{t},\overline{x}\right)=w\left(\overline{t},\overline{x}\right)$.
        \item $W$ is a \textit{viscosity supersolution} of \ref{4-1-newprogreq} in $Q$ if for every $w\in\mathcal{D}$:
        \begin{equation}\label{4-1-defsupersolution}
            -\frac{\partial}{\partial t}w\left(\overline{t},\overline{x}\right) + \left(\mathcal{G}_{\overline{t}}w\left(\overline{t},\cdot\right)\right)\left(\overline{x}\right) \geq 0,
        \end{equation} 
        at every:
        \[\left(\overline{t},\overline{x}\right)\in \arg \min_{(t,x)\in Q}\left\{(W-w)(t,x)\right\},\]

        and $W\left(\overline{t},\overline{x}\right)=w\left(\overline{t},\overline{x}\right)$.
        \item $W$ is a \textit{viscosity solution} if it is both a subsolution and a supersolution.
    \end{enumerate}
\end{definition}

As a first step after the definition of viscosity solutions, we prove its consistency with the classical notion. To do so we require the 
operator to satisfy a maximum principle, that is if $\mathcal{G}_t$ is a general 
operator and:

\[\mathcal{D}=\left\{W\in C([t_0,t_1]\times\Sigma)\,|\,W_t(t,x),(\mathcal{G}_tW(t,\cdot))(x)\in C(Q)\right\},\]

then $\mathcal{G}_t$ satisfies the \textit{maximum principle} if:

\[\mathcal{G}_t\phi(\overline{x})\geq\mathcal{G}_t\psi(\overline{x})\]

for every $\overline{x}\in\arg \max \{(\phi-\psi)(x)\,|\,x\in\Sigma\}\cap\Sigma'$ with $\phi(\overline{x})=\psi(\overline{x})$. Now,
if $W$ is a classical solution of \ref{4-1-newprogreq}, then for a $w\in\mathcal{D}$ and $\left(\overline{t},\overline{x}\right) \in \arg \max_{(t,x)\in Q}\left\{(W-w)(t,x)\right\}$ 
and $W\left(\overline{t},\overline{x}\right)=w\left(\overline{t},\overline{x}\right)$ then:

\begin{align}
    -\frac{\partial}{\partial t}w\left(\overline{t},\overline{x}\right) + \left(\mathcal{G}_{\overline{t}}w\left(\overline{t},\cdot\right)\right)\left(\overline{x}\right) \leq -\frac{\partial}{\partial t}W\left(\overline{t},\overline{x}\right) + \left(\mathcal{G}_{\overline{t}}W\left(\overline{t},\cdot\right)\right)\left(\overline{x}\right) = 0,
\end{align}

since we asked continuity of time derivatives. For the supersolution recall that:

\[ \max \{(\phi-\psi)(x)\,|\,x\in \Sigma\} = \min \{-(\phi-\psi)(x)\,|\,x\in \Sigma\}.\]

If $\mathcal{G}_t$ is the infinitesimal generator of a two-parameter semigroup the connection between classical and viscosity solutions is even stronger.

\begin{proposition}
    Let $W\in\mathcal{D}$. Then $W$ is a classical solution of \ref{4-1-newprogreq} if and only if it is a viscosity solution of \ref{4-1-newprogreq} in $Q$.

    \begin{proof}
        If $W$ is a viscosity solution, since it is also a test function then \ref{4-1-subsolution} and \ref{4-1-supersolution} hold for every point $(t,x)\in Q$, 
        which implies that:
        \[-\frac{\partial}{\partial t}w(t,x) + \left(\mathcal{G}_tw(t,\cdot)\right)(x) = 0,\,\forall(t,x)\in Q.\] 

        If $W$ is a classical solution and prove the subsolution property. Let $w\in\mathcal{D}$ and $(t,x)$ as usual. Since $w\geq W$:

        \begin{align*}
            -\frac{\partial}{\partial t}w(t,x) + (\mathcal{G}_tw(t,\cdot))(x) & = - \lim_{h\to0} \frac{1}{h}\left[(\mathcal{T}_{tt+h}w(t+h,\cdot))(x)-w(t,x)\right] \\
            & \leq - \lim_{h\to0} \frac{1}{h}\left[(\mathcal{T}_{tt+h}W(t+h,\cdot))(x)-W(t,x)\right] \\
            & = -\frac{\partial}{\partial t}W(t,x) + (\mathcal{G}_tW(t,\cdot))(x) = 0.
        \end{align*}

        The supersolution property is proven similarly.
    \end{proof}
\end{proposition}

We now prove that the family of linear operators $\mathcal{T}_{tt_1}$ defined in \ref{4-1-defofT_dynam} defines viscosity solution of the dynamic programming 
equation \ref{4-1-newprogreq}.

\begin{theorem}\label{4-1-thm:Vtoviscosity}
    Let $\{\mathcal{T}_{tr}\}_{t_0\leq t\leq r\leq t_1}$ such that \ref{4-1-identityofT},\ref{4-1-geqT},\ref{4-1-leqT} and also there exists a vector space 
    $\mathcal{D}$ and another family of operator $\{\mathcal{G}_t\}_{t\in[t_0,t_1]}$ such that \ref{4-1-conditiononD} and \ref{4-1-conditiononG_t} hold. Let:
    
    \[V(t,x)=\left(\mathcal{T}_{tt_1}\psi\right)(x).\]

    If $V\in C(Q)$ then it is a viscosity solution of \ref{4-1-newprogreq}.

    \begin{proof}
        Let us prove the subsolution condition. Let $w\in\mathcal{D}$ and $(t,x)$ maximizer of $V-w$ in $\overline{Q}$ and $V(t,x)=w(t,x)$. Then $V\leq w$ on 
        $Q$, which implies by \ref{4-1-geqT} that we have:
        
        \begin{equation}\label{4-1-proof1-firsstineq}(\mathcal{T}_{tr}w(r,\cdot))(x) \geq (\mathcal{T}_{tt_1}\psi)(x) = V(t,x) = w(t,x).\end{equation}

        Now, because of \ref{4-1-conditiononD} and \ref{4-1-conditiononG_t} we can compute:
        
        \[-\frac{\partial}{\partial t}w(t,x) + (\mathcal{G}_tw(t,\cdot))(x) = - \lim_{h\to0} \frac{1}{h}\left[(\mathcal{T}_{tt+h}w(t+h,\cdot))(x)-w(t,x)\right]\leq0\]

        because of \ref{4-1-proof1-firsstineq}. The supersolution condition is proven analogously.
    \end{proof}
\end{theorem}

\subsection{The Standard Approach}

We have talked about \textit{viscosity solutions} for $\mathcal{G}_t$ that are infinitesimal generators of semigroups. Historically the notion of viscosity solution was introduced 
for partial differential equations, that is when $\mathcal{G}_t$ is a partial differential operator. In this section we will introduce this classical notion and provide some properties, 
establishing as well the relevant links to the already proven theory. 

Let $\mathcal{G}_t$ be a partial differential operator defined As

\begin{equation}\label{4-1-GPDEop}
    (\mathcal{G}_t\phi)(x) = F(t,x,D\phi(x), D^2\phi(x), \phi(x)),
\end{equation}

where $F$ is taken to be continuous. We assume that $\sigma'=O\subset\R^n$ open and $\sigma$ to be its closure. 
Furthermore, we assume that test functions $\mathcal{C}$ and $\mathcal{D}$ are such that

\begin{equation}\label{4-1-condonCandD}
    C_p(\overline{O})\cap\mathcal{M}(\overline{O}) \subset \mathcal{C},\,C_p^{\infty}(\overline{Q})\cap\mathcal{M}(\overline{Q}) \subset \mathcal{D},
\end{equation}

where $Q=[t_0,t_1)\times O$ as usual. A useful result is the following.

\begin{lemma}
    The operator $\mathcal{G}_t$ obeys to the maximum principle if and only if $F$ is elliptic.

    \begin{proof}
        Recall that $F$ is elliptic If

        \begin{equation}\label{4-1-defelliptic}
            F(t,x,p,A+B,V) \leq F(t,x,p,A,V)
        \end{equation}

        for all $(t,x,p,V)\in Q\times\R^n\times\R$ and all $A,B\in Sym(\R^{n\times n})$ with $B\geq0$.

        Let us assume that $F$ is elliptic, then for generic $\phi,\psi\in C^2(O)$ and $\overline{x}\in\arg \max\{(\phi-\psi)(x)|x\in\overline{O}\}\cap O$ 
        with $\phi(\overline{x})=\psi(\overline{x})$ we have that $D\phi(\overline{x})=D\psi(\overline{x})$ and $D^2\phi(\overleftarrow{x})\leq D^2\psi(\overleftarrow{x})$, 
        therefore

        \[F(t,\overline{x}, D\phi(\overline{x}), D^2\phi(\overline{x}),\phi(\overline{x}))\geq F(t,\overline{x}, D\psi(\overline{x}), D^2\psi(\overline{x}),\psi(\overline{x})),\]

        just by using \ref{4-1-defelliptic} with $B=D^\psi(\overline{x})-D^2\phi(\overline{x})\geq0$. If $F$ is not elliptic 
        for some $(t,\overline{x},p,A,V)$ then the functions

        \[\psi(x) = V+p\cdot(x-\overline{x}) + \frac{1}{2}(A+B)(x-\overline{x})\cdot(x-\overline{x}),\]

        \[\phi(x) = V+p\cdot(x-\overline{x}) + \frac{1}{2}A(x-\overline{x})\cdot(x-\overline{x}),\]

        for $x\in\overline{O}$ are such that $\overline{x}$ maximizes the distance between them, they coincide over it, but 
        the value of $F$ in $\overline{x}$ and $\phi,\psi$ does not satisfy the maximum principle, since \ref{4-1-defelliptic} 
        does not hold.
    \end{proof}
\end{lemma}

I this context we give the following definition.

\begin{definition}\label{4-1-defviscforW}
    Given a function $W\in C(\overline{O})$ we define the equation
    \begin{equation}\label{4-1-PDEwithW}
        -\frac{\partial}{\partial t}W(t,x) + F(t,x,D_xW(t,x),D_x^2W(t,x),W(t,x))=0
    \end{equation}

    \begin{enumerate}[label=(\alph*)]
        \item $W$ is a \textit{viscosity subsolution} of \ref{4-1-PDEwithW} in $Q$ if for each $w\in C^{\infty}(Q)$ then 
        
        \begin{equation}
            -\frac{\partial}{\partial t}w(t,x) + F(t,x,D_xw(t,x),D_x^2w(t,x),w(t,x))\leq0
        \end{equation}

        at every $(t,x)\in Q$ which locally maximizes $W-w$ yielding $0$.
        
        \item $W$ is a \textit{viscosity supersolution} of \ref{4-1-PDEwithW} in $Q$ if for each $w\in C^{\infty}(Q)$ then 
        
        \begin{equation}
            -\frac{\partial}{\partial t}w(t,x) + F(t,x,D_xw(t,x),D_x^2w(t,x),w(t,x))\geq0
        \end{equation}

        at every $(t,x)\in Q$ which locally minimizes $W-w$ yielding $0$.

        \item $W$ is a \textit{viscosity solution} if it is both a viscosity subsolution and supersolution.
    \end{enumerate}
\end{definition}

We will prove that definitions \ref{4-1-defviscsol} and \ref{4-1-defviscforW} are equivalent for partial differential operators 
of the form \ref{4-1-GPDEop}, provided that the function has sub-polynomial growth and is measurable.

An handy way of verifying both the definitions is the following.

\begin{lemma}
    If the conditions in the definitions \ref{4-1-defviscsol}, \ref{4-1-defviscforW} is satisfied only at strict extrema 
    then it is satisfied at every extrema.

    \begin{proof}
        We prove it for the minimum of definition \ref{4-1-defviscforW}, the other assertions are proved similarly. Let 
        $(t,x)$ be a non strict minimum of $W-w$ such that $W(t,x)=w(t,x)$. Let $\epsilon>0$ and deifne $w^{\epsilon}=w+\epsilon\xi$ where
        
        \[\xi(s,y)=e^{-(\abs{s-t}^2+\abs{y-x}^2)}-1\]

        defined over $\overline{Q}$. Since $\mathcal{D}$ is a vector space and $\epsilon\xi\in\mathcal{D}$ by \ref{4-1-condonCandD}, then 
        $w^{\epsilon}\in\mathcal{D}$. Then for every $\epsilon>0$ the function $W-w^{\epsilon}$ has a strict minimum at $(t,x)$ with $W(t,x)=w^{\epsilon}(t,x)$ which 
        implies that
        
        \[-\frac{\partial}{\partial t}w^{\epsilon}(t,x) + F(t,x,D_xw^{\epsilon}(t,x),D_x^2w^{\epsilon}(t,x),w^{\epsilon}(t,x))\leq0.\]

        By continuity of $F$ and smoothness of $w^{\epsilon}$ we let $\epsilon\to0$ and get the wanted inequality.
    \end{proof}
\end{lemma}


Finally, we prove the equivalence of these definitions. 

\begin{theorem}
    Let all the previous assumptions and $W\in C_p(\overline{Q})\cap\mathcal{M}(\overline{Q})$ and $\mathcal{D}\subset C^{1,2}(Q)$. Then $W$ is a subsolution (supersolution) 
    of \ref{4-1-PDEwithW} as defined by \ref{4-1-defviscsol} if and only if it is a subsolution (supersolution) as defined by \ref{4-1-defviscforW}. 

    \begin{proof}
        Let $W$ be a supersolution as in \ref{4-1-defviscsol}. We just prove the supersolution property, since if we have $W$ subsolution we can see it as 
        a supersolution. Let $W$ a subsolution and $(t,x)$ a strict maximum of $W-w$ and $W(t,x)=w(t,x)$, then we can find an open set $A$ such that $w>W$ over 
        $A\cap Q$. By sub-polynomial growth there exists a constant $K$ such that 

        \[\abs{W(s,y)} \leq K(1+\abs{y}^{2m}),\,\forall (s,y)\in\overline{Q}\]

        for some $m>0$. Then for some $\xi$ given by Urysohn’s lemma we can define 

        \[\overline{w}(s,y)= \xi(s,y)w(s,y) - (1-\xi(s,y))K(1+\abs{y}^{2m}),\]

        and get a minimum for $W-\overline{w}$ in $(t,x)$. Then we are in the supersolution case.

        Let $w\in C^{\infty}(Q)$ and $(t,x)$ a local minimum of $W-w$ where it is null. There exists an open subset $\mathcal{N}$ of $\R^n$ such that 
        $W\geq w$ over $\mathcal{N}\cap\overline{Q}$, and by Urysohn’s lemma there exists a function $\xi\in C^{\infty}(\overline{Q})$ taking values in $[0,1]$, 
        constantly $1$ in a neighborhood $\tilde{\mathcal{N}}$ of $(t,x)$ and zero outside $\mathcal{N}$. Since $W$ is bounded below we define 
        
        \begin{equation}
            \tilde{w}(s,y) = \xi(s,y)w(s,y) - (1-\xi(s,y))K,
        \end{equation}

        where $-K$ is the lower bound of $W$. Then, by hypotheses on $W$, $w$, and $\xi$ we have

        \[W = \xi W  + (1-\xi) W \geq \xi w + (1-\xi)W\geq\xi w-(1-\xi)K=\overline{w}.\]

        Furthermore, $\overline{w}\in\mathcal{D}$ since \ref{4-1-condonCandD} holds, $(t,x)$ is a minimum of $W-\overline{w}$, and $W(t,x)=\overline{w}(t,x)$ 
        then 
        
        \[-\frac{\partial}{\partial t}\overline{w}(t,x) + F(t,x,D_y\overline{w}(t,x),D_y^2\overline{w}(t,x),\overline{w}(t,x))\geq0,\]

        but $\overline{w}$ and $w$ have same derivative in $(t,x)$, since $\xi$ is constantly unitary in a neighborhood of it. Then we get

        \[-\frac{\partial}{\partial t}w(t,x) + F(t,x,D_yw(t,x),D_y^2w(t,x),w(t,x))\geq0.\]

        Let us suppose that $W$ is a subsolution for \ref{4-1-defviscforW}. Let $w\in\mathcal{D}$ and $(t,x)$ a strict maximum of $W-w$ such that $(W-w)(t,x)=0$. 
        Since $\mathcal{D}\subset C^{1,2}(Q)$ we can find an open set $Q^{\ast}$ over which $w$ is $C^{1,2}$ and then extend it to be null over $\R^{n+1}\setminus Q^{\ast}$. 
        Let us define a sequence of mollifications of $w$ defined as $w^n$ such that $w^n,w_t^n,w_{x_i}^n,w_{x_ix_j}^n$ converge to their respective of $w$. 
        Since the convergence of $w^n$ to $w$ is uniform there must be a subsequence $(t_n,x_n)$ converging to $(t,x)$, such that $W-w^n$ has a maximum. If we set 
        $\overline{w}^n=w^n-w^n(t_n,x_n)+W(t_n,x_n)$ we still have a maximum in $(t_n,x_n)$ for $W-\overline{w}^n$ and $W(t_n,x_n)=\overline{w}^n(t_n,x_n)$. Then 

        \[-\frac{\partial}{\partial t}\overline{w}^n(t_n,x_n) + F(t_n,x_n,D_x\overline{w}^n(t_n,x_n), D_x^2\overline{w}^n(t_n,x_n), \overline{w}^n(t_n,x_n))\leq0.\]

        Because of continuity of $F$ and smoothness of $\overline{w}^n$ we get
        
        \[-\frac{\partial}{\partial t}w(t,x) + F(t,x,D_yw(t,x),D_y^2w(t,x),w(t,x))\leq0.\]
    \end{proof}
\end{theorem}

\subsection{Value Function as Viscosity Solution}

In the first section we proved that the value function of a dynamic programming problem is the classical solution of the dynamic programming equation, under differentiability 
assumptions. We now prove that it is a viscosity solution of the dynamic programming equation given two sets of assumptions.

\begin{theorem}\label{4-1-viscodynamiceq}
    Let $U$ be a bounded space of control and $f\in C(\overline{Q}\times U)$ such that $\abs{f(t,x,v)}\leq K(1+\abs{x})$. The for every $w\in C^1(Q)\cap \mathcal{M}(\overline{Q})$ 

    \begin{equation}
        \lim_{h\to 0} \frac{1}{h}\left[(\mathcal{T}_{tt+h}w(t+h,\cdot))(x)-w(t,x)\right] = \frac{\partial}{\partial t}w(t,x) - H(t,x,D_xw(t,x)), \,\forall(t,x)\in\overline{Q}.
    \end{equation}
\end{theorem}

\vspace{5mm}


Note that equation \ref{4-1-viscodynamiceq} implies that $V$ is a viscosity solution of \ref{1-2-HJB1} via theorem \ref{4-1-thm:Vtoviscosity}.

\vspace{5mm}

\begin{proof}

    Let $(t,x)\in Q$, $v\in U$ and $w\in C^1(Q)\cap\mathcal{M}(\overline{Q})$. We prove that
    
    \begin{equation}\label{4-1-inproof:limsup}\limsup_{h\to0}\frac{1}{h}\left[(\mathcal{T}_{tt+h}w(t+h,\cdot))(x)-w(t,x)\right]\leq\frac{\partial}{\partial t}w(t,x) - H(t,x,D_xw(t,x)),\end{equation}

    and 

    \begin{equation}\label{4-1-inproof:liminf}\liminf_{h\to0}\frac{1}{h}\left[(\mathcal{T}_{tt+h}w(t+h,\cdot))(x)-w(t,x)\right]\geq\frac{\partial}{\partial t}w(t,x) - H(t,x,D_xw(t,x)).\end{equation}
    
    Equation \ref{4-1-inproof:limsup} can be proven without assuming boundedness of the control space $U$ and sub-linearity of the flow $f$. Indeed, there exists an admissible control $u(\cdot)\in \mathcal{U}(t,x)$ 
    such that $\lim_{s\to t}u(s)=v$. Let $x(\cdot)$ be the state solution of the Cauchy problem associated 
    to the control $u(\cdot)$ such that $x(t)=x$. Let also $\tau$ the exit time of $(s,x(s))$ from $Q$. If we take a sufficiently small $h>0$ we can compute
    
    \begin{align*}
        \frac{1}{h}\left[(\mathcal{T}_{tt+h}w(t+h,\cdot))(x)-w(t,x)\right] & \leq \frac{1}{h}\int_t^{t+h}L(s,x(s),u(s))\,ds + \frac{1}{h}\left[w(t+h,x(t+h))-w(t,x)\right] \\
        & =  \frac{1}{h}\int_t^{t+h}\bigg[L(s,x(s),u(s)) \\ 
        & \quad + \frac{\partial}{\partial t}w(s,x(s)) + f(s,x(s),u(s))\cdot D_xw(s,x(s))\bigg]\,ds.
    \end{align*}
    
    Then \ref{4-1-inproof:limsup} holds true. 
    
    Let us prove \ref{4-1-inproof:liminf}. Using $\abs{f(t,x,v)}\leq K(1+\abs{x})$ we get

    \begin{align*}
        \abs{x(r)-x} & \leq \int_r^t \abs{f(s,x(s),u(s))} \,ds \\
        & \leq K(r-t) + K\int_t^r \abs{x}(s)\,ds \\
        & \leq K(r-t)(1+\abs{x}) + K\int_t^r \abs{x(s)-x}\,ds,
    \end{align*}

    which implies by Gronwall's inequality 

    \[\abs{x(r)-x} \leq (1+\abs{x})e^{K(t-r)-1},\,\forall r\geq t.\]

    Then for suitably small $r$ that $\tau\geq r\wedge t_1$ for all $u(\cdot)\in\mathcal{U}(t,x)$. Then for $n$ sufficiently large by definition of $\mathcal{T}_{tr}$ we can find a control $u^n(\cdot)$ such that
    
    \begin{align*}
        \left(\mathcal{T}_{tt+\frac{1}{n}}w\left(t+\frac{1}{n}\right)\right)(x) \geq \int_t^{t+1/n} L(s,x(s),u(s)) \,ds + w\left(t+\frac{1}{n},x^n\left(t+\frac{1}{n}\right)\right) - \frac{1}{n^2},
    \end{align*}

    which implies that 

    \begin{equation}\label{4-1-inproof:geq}
        \begin{aligned}
            n\left[\left(\mathcal{T}_{tt+\frac{1}{n}}w\left(t+\frac{1}{n}\right)\right)(x) - w(t,x)\right] & \geq \frac{\partial}{\partial t}w(t,x) + n\int_t^{t+1/n} L(s,x(s),u^n(s))\,ds \\ 
            & + n\int_t^{t+1/n} f(t,x,u^n(s))\,ds \cdot D_xw(t,x) + e(n),
        \end{aligned}        
    \end{equation}
    
    where 

    \begin{equation}
        \begin{aligned}
            e(n) = & -\frac{1}{n} +n\int_t^{t+1/n} \frac{\partial}{\partial t}\big(w(s,x^n(s)) - w(t,x)\big)\,ds \\
            & + n\int_t^{t+1/n} L((s,x^n(s),u^n(s))) - L(t,x,u^n(s))\,ds \\
            & + n\int_t^{t+1/n} f(s,x^n(s),u^n(s))\cdot D_xw(s,x^n(s)) - f(t,x,u^n(s))\cdot D_xw(t,x) \,ds.
        \end{aligned}
    \end{equation}

    Because of the continuity of $L$, of $f$ and smoothness of $w$ we have that $\lim_{n\to+\infty}e(n)=0$. If we prove that

    \begin{equation}\label{miao}
        n\int_t^{t+1/n} L(s,x(s),u^n(s))\,ds + n\int_t^{t+1/n} f(t,x,u^n(s))\,ds \cdot D_x w(t,x) \leq \sup_{v\in U}\left\{f(t,x,v)\cdot p + L(t,x,v)\right\},
    \end{equation}  

    then from \ref{4-1-inproof:geq} we get

    \[\liminf_{h\to0}\frac{1}{h}\left[\left(\mathcal{T}_{tt+h}w\left(t+h\right)\right)(x) - w(t,x)\right] \geq \frac{\partial}{\partial t}w(t,x) - H(t,x,D_x w(t,x)).\]
 
    \color{red}{PROVE THAT \ref{miao} VIA CONVEXHULL STUFF.}\color{black}{}
\end{proof}

It is important to notice that the previous result holds under quite stringent hypotheses. We both require the space control to be bounded and the flow $f$ to have sublinear growth in its state variable. We can relax these assumptions 
and get a similar result, but we need to require the existence of an optimal control.\

\begin{theorem}
    If for each $(t,x)\in Q$ there exists a $u^{\ast}\mathcal{U}(t,x)$ be an optimal control, then a continuous value function is a viscosity solution of its dynamic programming equation.
    
    \begin{proof}
        Let us drop the $^{\ast}$. As we did before in theorem \ref{4-1-viscodynamiceq}, for every $w\in C^{\infty}(Q)$ we have

        \[\limsup_{h\to0}\frac{1}{h}\left[(\mathcal{T}_{tt+h}w(t+h,\cdot))(x)-w(t,x)\right]\leq\frac{\partial}{\partial t}w(t,x) - H(t,x,D_xw(t,x)).\]

        If $V-w$ has a maximum at $(t,x)$ and $V(t,x)=w(t,x)$ then 

        \[\limsup_{h\to0}\frac{1}{h}\left[\underbrace{\int_t^{t_1} L(s,x(s),u(s))\,ds}_{=V(t,x)-w(t,x)=0} + w(t+h,x) - V(t+h,x)\right]=0,\]

        which proves subsolution. Let the subsolution hypotheses hold, then from the dynamic programming principle we get

        \begin{align*}
            0 & \geq \int_t^{t+h} L(s,x(s),u(s)) \,ds + w(t+h,x(t+h)) - w(t,x)\\ 
            & \geq  \int_t^{t+h} L(s,x(s),u(s)) + \frac{\partial}{\partial t}w(s,x(s)) + f(s,x(s),u(s))\cdot D_x w(s,x(s)) \,ds \\
            & \geq \int_t^{t+h} L(s,x(s),u(s)) - H(s,x(s),D_x w(s,x(s))) \,ds.
        \end{align*} 

        Finally, if we let $h\to0$ we get

        \[-\frac{\partial }{\partial t} w(t,x) + H(t,x,D_xw(t,x)) \geq 0.\]

    \end{proof}
\end{theorem}

We now turn our attention to uniqueness property of viscosity solution to the dynamic programming equation. Let us consider the dynamic programming equation 
in a more general form

\begin{equation}\label{4-2-firstordereq}
    -\frac{\partial}{\partial t}V(t,x) + H(t,x,D_xV(t,x)) = 0,\,(t,x)\in Q.
\end{equation}

We will show that under technical assumptions on $H$, which are satisfied by the Hamiltonian, the supremum norm of the difference between two viscosity solution 
can be computed only on the frontier. Therefore, if we impose boundary conditions we get uniqueness of the viscosity solution. 
The assumptions on $H$ are

\begin{itemize}\label{4-2-Hassumptions}
    \item There exists $K\in\R$ and $h\in C([0,\infty))$ with $h(0)=0$ such that $\forall (t,x),(s,y)\in\overline{Q}$ and $\forall p,p'\in\R^n$ we have
    
    \[H(t,x,p)-H(s,y,p')\leq h(\abs{t-s}+\abs{x-y})+h(\abs{t-s})\abs{p}+K\abs{x-y}\abs{p} + K\abs{p-p'}.\]
    
    \item There exists $K'\in\R$ such that
    
    \[\abs{H_t}+\abs{H_x}\leq K'(1+\abs{p}),\]

    and

    \[\abs{H_p}\leq K'.\]
\end{itemize}

Under this hypotheses we can state the following theorem.

\begin{theorem}\label{4-2-Theo:Suponbound}
    Let $W$ and $V$ viscosity subsolution and supersolution of \ref{4-2-firstordereq} in $Q$, respectively. If $Q$ is unbounded we assume $W$, $V$ to be bounded and 
    uniformly continuous on its closure. Then
    
    \[\sup_{\overline{Q}}[W-V] = \sup_{\partial^{\ast}Q}[W-V].\]
\end{theorem}

This Theorem implies that if $V$ and $W$ are two viscosity solution of \ref{4-2-firstordereq} satisfying the boundary conditions

\begin{equation}\label{4-2-TerminalcondV}
    \begin{aligned}
        V(t,x) = g(t,x),&\forall(t,x)\in[T_0,t_1)\times O, \\
        V(t_1,x) = \psi(x),&x\in\overline{O},
    \end{aligned}
\end{equation}

then, if the assumption of theorem \ref{4-2-Theo:Suponbound} are satisfied, we get

\[\norm{W-V}_{\infty} = \sup_{\overline{Q}}[W-V]=\sup_{\partial^{\ast}Q}[W-V] = 0.\]

The proof of Theorem \ref{4-2-Theo:Suponbound} is rather long; we first prove the $\overline{Q}$ bounded case and then the unbounded one.

\begin{proof}
    
    Let $Q$ be bounded. For $\epsilon, \delta, \beta >0$ define
    
    \begin{equation}\label{4-2-InProofSup:phi}
        \Phi(t,x,s,y) = W(t,x) - V(s,y) - \frac{1}{2\epsilon} \abs{x-y}^2 -\frac{1}{2\delta} \abs{t-s}^2 + \beta(s-t_1).   
    \end{equation}

    Since $\overline{Q}$ is bounded the function $\Phi$ achieves its supremum at $(t',x'),(s',y')\in\overline{Q}\times\overline{Q}$. We will essentially prove 
    that $\Phi$ divides $W-V$ from its supremum over the boundary in an inequality chain.
    
    \begin{enumerate}
        \item Let $\rho>$ and define
        
        \begin{equation}\label{4-2-InProofSup:Drho}
            D_{\rho} = \left\{(t,x),(s,y)\in\overline{Q}\times\overline{Q}\,|\,\abs{t-s}^2+\abs{x-y}^2\leq\rho\right\},
        \end{equation}

        and 

        \[m_W(\rho)=2\left\{\abs{W(t,x)-W(s,y)}:\,((t,x),(s,y))\in D_{\rho}\right\},\]

        \[m_V(\rho)=2\left\{\abs{V(t,x)-V(s,y)}:\,((t,x),(s,y))\in D_{\rho}\right\},\]

        and 

        \[K_1 = \sup\{m_w(\rho):\,\rho\geq0\}.\]

        Since $\overline{Q}$ is compact and $W$,$V$ are continuous they are uniformly continuous, therefore $m_W,m_V\in C([0,\infty))$. Clearly 
        $m_W(0)=m_V(0)=0$. We now claim two estimates on $\abs{x-y}$ and $\abs{t-s}$:

        \begin{equation}
            \begin{aligned}
                \abs{t'-s'}\leq\sqrt{K_1\delta} \\
                \abs{x'-y'}\leq \sqrt{\epsilon m_W(K_1[\epsilon+\delta])}.
            \end{aligned}
        \end{equation}

        From the maximum in $((t',x'),(s',y'))$ we have

        \[\Phi(t',x',s',y') \leq \Phi(s',y',s',y'),\]

        implies

        \[\frac{1}{\epsilon}\abs{x'-y'}^2 + \frac{1}{\delta}\abs{t'-s'}^2\leq 2(W(t',x')-W(s',y')) \leq m_W(\abs{t'-s'}+\abs{x'-y'}).\]

        Then we have

        \[\frac{1}{\delta}\abs{t'-s'}^2\leq K_1,\,\frac{1}{\epsilon}\abs{x'-y'}^2\leq K_1,\]

        which also imply

        \[\frac{1}{\epsilon}\abs{x'-y'}^2\leq m_w(K_1[\epsilon+\delta]).\]

        \item We now show that if $(t',x')\in\partial^{\ast}Q$ or $(s',y')\in\partial^{\ast}Q$ we have
        
        \begin{equation}\label{4-2-InProofSup:phiestim1}
            \Phi(t',x',s',y') \leq\frac{1}{2}m_V(K_1(\epsilon+\delta)) + \sup_{\partial^{\ast}Q}[W-V],
        \end{equation}

        or 

        \begin{equation}\label{4-2-InProofSup:phiestim2}
            \Phi(t',x',s',y') \leq\frac{1}{2}m_W(K_1(\epsilon+\delta)) + \sup_{\partial^{\ast}Q}[W-V],
        \end{equation}

        respectively. Indeed, in the first case, we have

        \begin{align*}\Phi(t',x',s',y')&\leq W(t',x') - W(s',y') \\ & \leq  V(t',x') - V(s',y') + \sup_{\partial^{\ast}Q}[W-V] \\
        & \leq \frac{1}{2}m_W(\abs{t'-s'}^2+\abs{x'-y'}^2) +  \sup_{\partial^{\ast}Q}[W-V]. \end{align*}

        Same reasoning for the latter.

        \item We now construct an estimate over the parameter $\beta$ if $(t',x'),(s',y')\in Q$. Let us consider the test function
        
        \[w(t,x) = \frac{1}{2\delta}\abs{t-s'}^2 + \frac{1}{2\epsilon}\abs{x-y'}^2,\]

        then $(t',x')$ is where $W-w$ is maximized and $W(t',x')=w(t',x')$, then

        \begin{equation}\label{4-2-InProofSup:qandp1}-\underbrace{\frac{1}{\delta}(t'-s')}_{\overset{def}{=}q_{\delta}} + H(t',x',\underbrace{\frac{1}{\epsilon}(x'-y')}_{\overset{def}{=}p_{\epsilon}})\leq0.\end{equation}

        Analogously, defining 

        \[w(s,y) = -\frac{1}{2\delta}\abs{t'-s}^2 - \frac{1}{2\epsilon}\abs{x'-y}^2 + \beta(s-t_1),\]

        we get:

        \begin{equation}\label{4-2-InProofSup:qandp2}
            -\beta-q_{\delta}+H(s',y',p_{\epsilon})\geq0.
        \end{equation}

        Combining \ref{4-2-InProofSup:qandp1} and \ref{4-2-InProofSup:qandp2} we get

        \begin{align*}
            \beta & \leq H(s',y',p_{\epsilon}) - H(t',x',p_{\epsilon}) \\
            & \leq h(\abs{t'-s'} + \abs{x'-y'}) + h(\abs{t'-s'})\abs{p_{\epsilon}} + K\abs{x'-y'}\abs{p_{\epsilon}} \\
            & \leq h(\sqrt{K_1\epsilon} + \sqrt{K_1\delta}) + h(\sqrt{K_1\delta})\frac{1}{\epsilon}\sqrt{K_1\epsilon} + K\frac{\abs{x'-y'}^2}{\epsilon} \\
            & \leq h(\sqrt{K_1\epsilon} + \sqrt{K_1\delta}) + h(\sqrt{K_1\delta})\sqrt{\frac{K_1\epsilon}{\epsilon}} + Km_W(K_1[\epsilon+\delta]).
        \end{align*}
        
        \item We denote
        
        \begin{equation}\label{4-2-inproofSup:k}
            k(\epsilon,\delta) = h(\sqrt{K_1\epsilon} + \sqrt{K_1\delta}) + h(\sqrt{K_1\delta})\sqrt{\frac{K_1\epsilon}{\epsilon}} + Km_W(K_1[\epsilon+\delta]),
        \end{equation}

        then

        \[\lim_{\epsilon\to0}\lim_{\delta\to0} k(\epsilon,\delta) = 0.\]

        Then for suitably small $\epsilon$ and $\delta$ we have

        \[k(\epsilon,\delta) < \beta,\]

        which implies that either $(t',x')$ or $(s',y')$ is in $\partial^{\ast}Q$. Then by \ref{4-2-InProofSup:phiestim1} 
        or \ref{4-2-InProofSup:phiestim2} we have

        \begin{equation}\label{4-2-InProofSup:limsuplimsup}
            \limsup_{\epsilon\to0}\limsup_{\delta\to0} \Phi(t',x',s',y') \leq \sup_{\partial^{\ast}Q}[W-V],
        \end{equation}

        for every $\beta>0$. Finally, we have

        \[W(t,x) - V(s,y) + \beta(t-t_1) = \Phi(t,x,t,x) \leq \Phi(t',x',s',y'),\]

        then by \ref{4-2-InProofSup:limsuplimsup} we get

        \[W(t,x) - V(s,y) = \lim_{\beta\to0}W(t,x) - V(s,y) + \beta(t-t_1)\leq \lim_{\beta\to0}\Phi(t',x',s',y')\leq \sup_{\partial^{\ast}Q}[W-V].\]
    \end{enumerate}

    Let us suppose that $Q$ is unbounded, and $V,W$ are bounded and uniformly continuous. We keep the same notation as before. For every $\gamma>0$ 
    there exist $(t_{\gamma},x_{\gamma}),(s_{\gamma},y_{\gamma})\in \overline{Q}$ such that
    
    \begin{equation}
        \Phi(t_{\gamma},x_{\gamma},s_{\gamma},y_{\gamma}) \geq \sup_{\overline{Q}\times\overline{Q}}\Phi - \gamma,
    \end{equation}

    since $W$ and $V$ are bounded. We therefore define

    \begin{equation}
        \Phi_{\gamma}(t,x,s,y) = \Phi(t,x,s,y) - \frac{\gamma}{2}\left[\abs{t-t_{\gamma}}^2+\abs{s-s_{\gamma}}^2 + \abs{x-x_{\gamma}}^2 + \abs{y-y_{\gamma}}^2\right],
    \end{equation}

    for $(t,x),(s,y)\in \overline{Q}$. 
    
    We now follow a similar procedure to the one used in the bounded case.

    \begin{enumerate}[label=\arabic*')]
        \item Let the moduli of continuity $m_W,m_V$ and the bound $K_1$ as before. Since we assume the value functions to be bounded and 
        absolutely continuous we have, as before, that
        
        \[m_W,m_V\in C([0,\infty)),\, m_W(0)=m_V(0)=0,\, K_1=\sup\{m_W(\rho):\,\rho\geq0\}<+\infty.\]

        Let us note that for $(t,x),(s,y)\in\overline{Q}\times\overline{Q}$ such that 

        \[\abs{t-t_{\gamma}}^2+\abs{s-s_{\gamma}}^2 + \abs{x-x_{\gamma}}^2 + \abs{y-y_{\gamma}}^2>2,\]

        then

        \[\Phi_{\gamma}(t,x,s,y) \leq \Phi(t,x,s,y) - \gamma \leq \Phi(t_{\gamma},x_{\gamma},s_{\gamma},y_{\gamma}) = \Phi_{\gamma}\Phi(t_{\gamma},x_{\gamma},s_{\gamma},y_{\gamma}).\]

        The supremum of $\Phi_{\gamma}$ is achieved since $\Phi_{\gamma}$ is bounded by $\Phi(t_{\gamma},x_{\gamma},s_{\gamma},y_{\gamma})$. It is achieved at $(t',x'),(s',y')$. Then

        \begin{equation}
            \abs{t-t_{\gamma}}^2+\abs{s-s_{\gamma}}^2 + \abs{x-x_{\gamma}}^2 + \abs{y-y_{\gamma}}^2\leq2.
        \end{equation}

        We now claim two estimates on the distance between the time variables and space variables. For $\gamma,\delta,\epsilon\leq\frac{1}{2}$ we have

        \begin{equation}\label{4-2-inproofSup:unboundedCLAIMS}
            \abs{t'-s'} \leq \sqrt{2(K_1+1)\delta},\,\abs{x'-y'}\leq\sqrt{2\epsilon(2\gamma + m_W(2\epsilon(K_1+1)))}.
        \end{equation}

        Indeed, $\Phi_{\gamma}(t',x',s',y')\geq\Phi_{\gamma}(s',x',s',y')$ implies

        \begin{align*}
            \frac{1}{2\delta}\abs{t'-s'}^2 & \leq W(t',x')-W(s',x') + \frac{\gamma}{2}[\abs{s'-t_{\gamma}}^2-\abs{t'-t_{\gamma}}^2] \\
            & \leq \frac{1}{2}K_1 + \gamma\abs{t'-s'}^2 + \frac{\gamma}{2}\abs{t'-t_{\gamma}}^2 \\
            & \leq \frac{1}{2}K_1 + \gamma\abs{t'-s'}^2 + \gamma,
        \end{align*}

        which implies, recalling the bound on $\delta$ and $\gamma$, that 

        \[\left(\frac{1}{4\delta}\right)\abs{t'-s'}^2\leq\left(\frac{1-\delta}{2\delta}\right)\abs{t'-s'}^2\leq\left(\frac{1}{2\delta}-\gamma\right)\abs{t'-s'}^2\leq \frac{1}{2}K_1 + \frac{1}{2}.\]

        From $\Phi_{\gamma}(t',x',s',y')\geq\Phi_{\gamma}(t',y',s',y')$ we get

        \begin{align*}
            \frac{1}{2\epsilon}\abs{x'-y'}^2 & \leq W(t',x')-W(t',x') + \frac{\gamma}{2}[\abs{y'-x_{\gamma}}^2-\abs{x'-x_{\gamma}}^2] \\
            & \leq \frac{1}{2}m_W(\abs{x'-y'}^2) + \gamma\abs{x'-y'}^2 + \frac{\gamma}{2}\abs{x'-x_{\gamma}}^2 \\
            & \leq \frac{1}{2}m_W(\abs{x'-y'}^2) + \gamma\abs{x'-y'}^2 + \gamma,
        \end{align*}

        which implies

        \[\frac{1}{4\epsilon}\abs{x'-y'}^2\leq \frac{1}{2}m_W(\abs{x'-y'}^2)+\gamma\Rightarrow \abs{x'-y'}^2\leq 2(K_1+1)\epsilon.\]

        Therefore, we get

        \[\frac{1}{4\epsilon}\abs{x'-y'}^2\leq \left(\frac{1}{2\epsilon}-\gamma\right)\abs{x'-y'}^2\leq \frac{1}{2}m_W\left(2(K_1+1)\epsilon\right)+\gamma,\]

        hence the second equation in \ref{4-2-inproofSup:unboundedCLAIMS}.

        \item Replicating the same argument of the bounded case we get if $(t',x')\in\partial^{\ast}Q$ then
        
        \begin{equation}
            \Phi_{\gamma}(t',x',s',y') \leq \frac{1}{2}m_V\left(2(K_1+1)(\epsilon+\delta)\right) + \sup_{\partial^{\ast}Q}[W-V],
        \end{equation}

        and if $(s',y')\in\partial^{\ast}Q$ then

        \begin{equation}
            \Phi_{\gamma}(t',x',s',y') \leq \frac{1}{2}m_W\left(2(K_1+1)(\epsilon+\delta)\right) + \sup_{\partial^{\ast}Q}[W-V].
        \end{equation}

        \item Finally, if we find a bound over $\beta$ for $(t',x'),(s',y')\in Q$ we then prove that this bound is not satisfied and then we can invoke 
        point $2'$ and get

        \begin{equation}
            \limsup_{\epsilon\to0}\limsup_{\delta\to0}\limsup_{\gamma\to0}\Phi_{\gamma}(t',x',s',y') \leq \sup_{\partial^{\ast}Q}[W-V],
        \end{equation}

        which implies, as before, that

        \[W(t,x) - V(s,y)\leq \sup_{\partial^{\ast}Q}[W-V].\]

        Let $(t',x'),(s',y')\in Q$ and the test function

        \[\overline{w}(t,x)=\frac{1}{2\delta}\abs{t-s'}^2+\frac{1}{2\epsilon}\abs{x-y'}^2+\frac{\gamma}{2}\left(\abs{t-t_{\gamma}}^2+\abs{x-x_{\gamma}}^2\right),\]

        defined over $Q$. Then $(t',x')$ is where $W-\overline{w}$ is null and maximized. Therefore

        \begin{equation}\label{4-2-inproofSup:qandpwithoutbeta}
             - \underbrace{\frac{1}{\delta}(t'-s')}_{\overset{def}{=}q_{\delta}}  - \underbrace{\gamma(t'-t_{\gamma})}_{\overset{def}{=}q_{\gamma}} + H(t',x',p_{\epsilon}+p_{\gamma}) \leq 0, 
        \end{equation}

        where

        \[p_{\epsilon}=\frac{1}{\epsilon}(x'-y'),\, p_{\gamma}=\gamma(x'-x_{\gamma}).\]

        With the same reasoning we define the test function

        \[w^{\ast}(s,y)=-\frac{1}{2\delta}\abs{t'-s}^2-\frac{1}{2\epsilon}\abs{x'-y}^2+\frac{\gamma}{2}\left(\abs{s-s_{\gamma}}^2+\abs{y-y_{\gamma}}^2\right)+\beta(s-t_1),\]

        from which we get

        \begin{equation}\label{4-2-inproofSup:qandpwithbeta}
            - \beta - q_{\delta}  - \underbrace{\gamma(s'-s_{\gamma})}_{\overset{def}{=}\overline{q}_{\gamma}} + H(s',y',p_{\epsilon}+\overline{p}_{\gamma}) \geq 0, 
       \end{equation}

       where $\overline{p}_{\gamma} = \gamma(y_{\gamma}-y').$

       We can find the upper bound to $\beta$ using the estimates in \ref{4-2-inproofSup:unboundedCLAIMS} and equations \ref{4-2-inproofSup:qandpwithbeta} and \ref{4-2-inproofSup:qandpwithoutbeta}:

       \begin{align*}
       \beta&\leq H(s',y',p_{\epsilon}+\overline{p}_{\gamma}) - H(t',x',p_{\epsilon}+p_{\gamma}) + q_{\gamma} - \overline{q}_{\gamma} \\
       & \leq h(\abs{t'-s'}+\abs{x'-y'})+h(\abs{t'-s'})\left[\abs{p_{\epsilon}+\abs{\overline{p}_{\gamma}}}\right] \\
       & + K\abs{x'-y'}\left[\abs{p_{\epsilon}+\abs{\overline{p}_{\gamma}}}\right]+K\abs{\overline{p}_{\gamma}-p_{\gamma}}+q_{\gamma}-\overline{q}_{\gamma} \\
       & \leq h(\sqrt{2(K_1+1)\delta}+\sqrt{2(K_1+1)\epsilon})+h(\sqrt{2(K_1+1)\delta})\left[\sqrt{\frac{\abs{2(K_1+1)}}{\epsilon}}+2\gamma\right] \\
       & + K2\left[2\gamma+m_W(2(K_1+1))\epsilon+\sqrt{2(K_1+1)\epsilon\delta}\right]+2(K+1)\gamma.
       \end{align*}
    \end{enumerate}
\end{proof}

\subsection{Continuity Properties of the Value Function}

We now investigate continuity properties of the value function. In particular, we consider two different sets of assumptions which guarantee continuity, actually 
something stronger: Lipschitz continuity and Uniform continuity. Let us consider $f,L,\psi$ as usual, recalling that there exists $K_{\rho}>0$ such that 

\begin{equation}\label{4-3-boundonf}
    \abs{f(t,x,v) - f(t,y,v)} \leq K_{\rho}\abs{x-y}, 
\end{equation}

for all $\abs{v}\leq\rho$. Note that these are minimal assumption to start working with the optimal control problem.

\begin{theorem}\label{4-3-Theorem:1cont}
    Let a bounded control space $U$, $Q=[t_0,t_1)\times\R^n$. Assume that $f,L,\psi$ are bounded, $f$ satisfies \ref{4-3-boundonf} and $L,\psi$ uniformly continuous. 
    Then the value function $V$ is bounded and uniformly continuous.
    
    \begin{proof}
        $V$ is bounded. Indeed 

        \[V(t,x) = \inf_{u\in\mathcal{U}}\int_t^{t_1} L(s,x(s),u(s))\,ds + \psi(t_1,x(t_1)) \leq (t_1-t)K_L + K_{\psi},\]

        where $K_g$ is the bound over the function $g$. 

        $V$ is uniformly continuous. Standard arguments on solutions of ODE (integral form of the solution, bound on the flow and Gronwall) yield 

        \begin{equation}
            \Lambda(s) \overset{def}{=} \abs{x(s)-y(s)} \leq \abs{x-y}e^{K(s-t)},
        \end{equation} 

        where $x(s),y(s)$ are solution to the Cauchy problem

        \[\frac{d}{ds}w(s) = f(s,w(s),u(s)),\, w(t)=w.\]

        Then

        \begin{align*}
            \abs{J(t,x,u) - J(t,y,u)} & \leq \int_t^{t_1} \abs{L(s,x(s),u(s)) - L(s,y(s), u(s))}\,ds + \abs{\psi(x(t_1)) - \psi(y(t_1))} \\
            & \leq \int_t^{t_1} m_L(\Lambda(s))\,ds + m_{\psi}(\Lambda(t_1)) \\
            & + (t_1-t_0)m_L(\abs{x-y}e^{K(t_1-t_0)}) + m_{\psi}(\abs{x-y}e^{K(t_1-t_0)}),
        \end{align*}

        where $m_g$ is the modulus of continuity of the function $g$. Since both $L$ and $\psi$ are uniformly continuous $m_L,m_{\psi}\in C([0,\infty))$ and 
        they are null at the origin. Then for a fixed $t$ we have the bound
        
        \begin{equation}\label{4-3-InProof:modulust}
            \abs{V(t,x)-V(t,y)} \leq (t_1-t_0)m_L(\abs{x-y}e^{K(t_1-t_0)}) + m_{\psi}(\abs{x-y}e^{K(t_1-t_0)}).
        \end{equation}

        Similarly, if we fix $x$ we get 

        \begin{equation}\label{4-3-InProof:modulusx}
            \abs{V(t,x)-V(s,x)} \leq K_1\abs{t-s} + (t_1-t_0)m_L(\abs{t-s}K_2e^{K(t_1-t_0)}) + m_{\psi}(\abs{t-s}K_2e^{K(t_1-t_0)}),
        \end{equation}

        where $K_1=\sup\abs{L}$ and $K_2$ is such that $\abs{x(s)-x}\leq K_2\abs{t-s}$, which exists because $f$ is bounded. Then by splitting 
        $\abs{V(t,x)-V(s,y)}$ we find a modulus of continuity for it, by summing \ref{4-3-InProof:modulusx} and \ref{4-3-InProof:modulust}. 
    \end{proof}
\end{theorem}

From the proof it is clear that by taking $L$ and $\psi$ Lipschitz continuous we get $V$ Lipschitz continuous, since we can use 

\[m_L(\rho)=K_L\rho,\, m_{\psi}(\rho)=K_{\psi}\rho,\]

for some constants $K_L,K_{\psi}>0$. 

This theorem allows us to conclude the following result.

\begin{corollary} 
    Under the previous assumptions, the value function is the unique viscosity solution of the dynamic programming equation with fixed terminal conditions \ref{4-2-TerminalcondV}. 

    \begin{proof} 
        Since $U$ is bounded the Hamiltonian $H$ satisfies \ref{4-2-Hassumptions}. Indeed, the bound on the derivative is obvious. 
        The upper bound function $h$ can be found as follows:

        \begin{align*}
            H(t,x,p) - H(s,y,p') & \leq \sup_{v\in U}\left\{\abs{L(t,x,v)-L(s,y,v) + f(t,x,v)\cdot p - f(s,y,v)\cdot p'}\right\} \\
            & \leq \sup_{v\in U}\left\{\abs{L(t,x,v)-L(s,y,v)}\right\} + \sup_{v\in U}\abs{f(s,y,v)}\abs{p-p'} \\
            & + \sup_{v\in U}\abs{f(t,x,v)-f(s,y,v)}\abs{p},
        \end{align*}

        but the uniform continuity of $L$ and Lipschitz continuity of $f$ yield \ref{4-2-Hassumptions}. Then by Theorem \ref{4-2-Theo:Suponbound} we get the thesis.
    \end{proof} 
\end{corollary}

% Let us now address the bounded $Q$ case. We assume that the value function has the form

% \[V(t,x) = \inf_{u\in\mathcal{U}} J(t,x;u),\]

% where 

% \[J(t,x;u) = \int_t^{\tau} L(s,x(s),u(s)) \,ds + \Psi(\tau,x(\tau)).\]

% We require $\Psi$ to be 

% \[\Psi(\tau,x(\tau)) = 0\text{ if }\tau<t_1,\,\Psi(\tau,x(\tau)) = \psi(x(t_1))\text{ if }\tau = t_1.\]

% \begin{theorem}
    
% \end{theorem}


\subsection{Pontryagin's Principle for Viscosity Solutions}

In section \ref{Section: Pontryagin Principle} we stated the Pontryagin's principle for continuously differentiable value functions. 
As we come to understand in this chapter, value functions may easily fail to be $C^1$ which requires a weaker notion of solution. We have introduced 
the concept of viscosity solution, which turns out to be also a handy notion to extend Pontryagin's principle in non-differentiable settings. 
Recall that it states the relation between a so-called adjoint variable $P$ and the state variable via a system of differential equations. Since may fail 
to be differentiable somewhere we need to carefully extend the possible values of the adjoint variable for those instances. Let us 
define two new key concepts that incorporate the possible value that the incremental limit may have in non-smooth points. 

\begin{definition}
    Let $W\in C(\overline{Q})$ and $(t,x)\in Q$. We define:

    \begin{enumerate}
        \item The set of \textit{superdifferentials} of $W$ at $(t,x)$ as the collection of all $(q,p)\in\R\times\R^n$ such that 
        there exists some $w\in C^1(Q)$ for which:
        
        \begin{equation}
            (q,p) = \left(\frac{\partial}{\partial t}w(t,x),D_x w(t,x)\right),
        \end{equation}

        and

        \begin{equation}
            (t,x) \in \arg \max \left\{(W-w)(s,y)\,|\,(s,y)\in\overline{Q}\right\}.
        \end{equation}

        We denote it with $D^+W(t,x)$.

        \item The set of \textit{subdifferentials} of $W$ at $(t,x)$ as the collection of all $(q,p)\in\R\times\R^n$ such that 
        there exists some $w\in C^1(Q)$ for which:
        
        \begin{equation}
            (q,p) = \left(\frac{\partial}{\partial t}w(t,x),D_x w(t,x)\right),
        \end{equation}

        and

        \begin{equation}
            (t,x) \in \arg \min \left\{(W-w)(s,y)\,|\,(s,y)\in\overline{Q}\right\}.
        \end{equation}

        We denote it with $D^-W(t,x)$.
    \end{enumerate}
\end{definition}

Note that this definition is consistent with the notion of viscosity solution since if $W$ is a viscosity subsolution in $Q$ 
if and only if 

\[-q+H(t,x,p)\leq 0,\,\forall (q,p)\in D^+W(t,x),\,\forall(t,x)\in Q,\]

and similarly for a supersolution.

The notion of superdifferential will take the place of the derivative in Pontryagin's result. We recall the definition of the adjoint variable for a state variable $x$, a control $u$, 
a terminal condition $\psi$ and a Hamiltonian $H$:

\begin{equation}\label{4-3-pontrydotp}
    \dot{p}^{\ast}(s) =  - D_x H(s,x^{\ast}(s),u^{\ast}(s),p^{\ast}(s)),
\end{equation}

And also:

\begin{equation}\label{4-3-pontry-maxH}
    H(s,x^{\ast}(s),u^{\ast}(s),p^{\ast}(s)) = \sup_{v\in U} H(s,x^{\ast}(s),v,p^{\ast}(s)),
\end{equation}

With:

\begin{equation}\label{4-3-pontryp}
    p^{\ast}(t_1) = D \psi(x^{\ast}(t_1)).
\end{equation}

We can now state Pontryagin's maximum principle in this broader context.

\begin{theorem}
    Let $u^{\ast}(\cdot)$ be an optimal control at $(t,x)$ which is right continuous at each $[t,t_1)$, and $p^{\ast}(s)$ defined by \label{4-3-pontrydotp}, \ref{4-3-pontry-maxH} and \ref{4-3-pontry-tras}. 
    Then for each $s\in[t,t_1)$ 
    
    \begin{equation}
        \bigg(H(s,x^{\ast},p^{\ast}(s)),p^{\ast}(s)\bigg) \in D^+V(s,x^{\ast}(s)).
    \end{equation}

    \begin{proof}
        By definition of value function we have

        \[V(r,y) \leq J(r,y;u^{\ast}),\,\forall (r,y)\in Q,\]

        and equality for $(r,y)=(s,x^{\ast}(s))$ for each $s\in[t,t_1]$. Hence, if we show that $J$ is continuously differentiable and

        \begin{equation}
            \frac{\partial}{\partial r}J(s,x^{\ast}(s);u^{\ast}) = p^{\ast}(s),
        \end{equation}

        and 

        \begin{equation}
            D_yJ(s,x^{\ast}(s);u^{\ast}) = H(s,x^{\ast}(s),p^{\ast}(s))
        \end{equation}

        for $s\in[t,t_1)$, then the thesis holds by definition. 
    \end{proof}
\end{theorem}






Poi II.10 fare bene Lipschitz.

Ci vuole importante e chiara spiegazione su perchè essere lipshizt sia fondamentale.

Poi II.15 Prontryagin.

La discussione sulle condizioni al constorno e robe varie boh. Vediamo come procede. Le cose prima necessarie.  